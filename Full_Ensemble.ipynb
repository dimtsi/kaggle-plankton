{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math;\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import importlib\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from torchvision.models.resnet import *\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "\n",
    "import NNs\n",
    "from NNs import *\n",
    "importlib.reload(NNs)\n",
    "import math\n",
    "from NNs import ResNetDynamic, FeatureBoostedCNN\n",
    "\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "from torchsummary import summary\n",
    "from Preprocessing import *\n",
    "from Preprocessing import ListsTrainDataset, ListsTestDataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/dimtsi/Dropbox/UvA/1st Semester/Applied Machine Learning/Project', '/home/dimtsi/anaconda3/envs/torch/lib/python37.zip', '/home/dimtsi/anaconda3/envs/torch/lib/python3.7', '/home/dimtsi/anaconda3/envs/torch/lib/python3.7/lib-dynload', '', '/home/dimtsi/.local/lib/python3.7/site-packages', '/home/dimtsi/anaconda3/envs/torch/lib/python3.7/site-packages', '/home/dimtsi/anaconda3/envs/torch/lib/python3.7/site-packages/IPython/extensions', '/home/dimtsi/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_images = pickle.load(open(\"pkl/extraclassified_padded64.pkl\", \"rb\"))\n",
    "# train_images = train_images[:1000]\n",
    "original_labels = pickle.load(open(\"pkl/extraclassified_train_labels.pkl\", \"rb\"))\n",
    "kaggle_test_images = pickle.load(open(\"pkl/test_padded64.pkl\", \"rb\"))\n",
    "kaggle_test_filenames = pickle.load(open(\"pkl/test_filenames.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load handcrafted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_haralick = pickle.load(open(\"features/extraclassified/train_haralick.pkl\", \"rb\"))\n",
    "original_moments = pickle.load(open(\"features/extraclassified/train_moments.pkl\", \"rb\"))\n",
    "original_sizes = pickle.load(open(\"features/extraclassified/train_sizes.pkl\", \"rb\"))\n",
    "\n",
    "kaggle_test_haralick = pickle.load(open(\"features/extraclassified/test_haralick.pkl\", \"rb\"))\n",
    "kaggle_test_moments = pickle.load(open(\"features/extraclassified/test_moments.pkl\", \"rb\"))\n",
    "kaggle_test_sizes = pickle.load(open(\"features/extraclassified/test_sizes.pkl\", \"rb\"))\n",
    "\n",
    "train_handcrafted_features = np.concatenate([original_haralick, original_moments,  original_sizes], axis =1)\n",
    "kaggle_test_handcrafted_features = np.concatenate([kaggle_test_haralick, kaggle_test_moments,  kaggle_test_sizes], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split to train test mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_mine_indexes = pickle.load(open(\"pkl/test_set_mine_indexes_extraclassified.pkl\", \"rb\"))\n",
    "\n",
    "train_images = [i for j, i in enumerate(original_train_images) if j not in test_set_mine_indexes]\n",
    "train_labels = [i for j, i in enumerate(original_labels) if j not in test_set_mine_indexes]\n",
    "train_handcrafted = [i for j, i in enumerate(train_handcrafted_features) if j not in test_set_mine_indexes]\n",
    "\n",
    "#\n",
    "test_mine_images = [i for j, i in enumerate(original_train_images) if j in test_set_mine_indexes]\n",
    "test_mine_labels = [i for j, i in enumerate(original_labels) if j in test_set_mine_indexes]\n",
    "test_mine_handcrafted = [i for j, i in enumerate(train_handcrafted_features) if j in test_set_mine_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn, y_train_cnn = train_images, train_labels\n",
    "X_val_cnn, y_val_cnn = test_mine_images, test_mine_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = resnet50(pretrained = True)\n",
    "cnn = ResNetDynamic(pretrained.block, pretrained.layers, num_layers = 2, pretrained_nn = None)\n",
    "#\n",
    "cnn_dict = torch.load('models/extraclassified/trained_model90.pt', map_location={\"cuda:1\": \"cuda:0\", \"cuda:2\": \"cuda:0\"})['state_dict']\n",
    "cnn.load_state_dict(cnn_dict)\n",
    "cnn = cnn.eval().cuda()\n",
    "del(pretrained)\n",
    "feature_extractor_cnn = nn.Sequential(*list(cnn.children())[:-2]).eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_norm_test, std_norm_test = calc_means_stds(train_images)\n",
    "\n",
    "def get_cnn_features(feature_extractor, model, x):\n",
    "    features = ...\n",
    "    mean_norm_test, std_norm_test = calc_means_stds(train_images)\n",
    "\n",
    "    test_transforms = transforms. Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[mean_norm_test],\n",
    "                    std =[std_norm_test])\n",
    "    ])\n",
    "    \n",
    "    total_features = torch.Tensor().float().cpu()\n",
    "    total_predicted = torch.Tensor().long()\n",
    "    total_probabilities = torch.Tensor().float()  \n",
    "    \n",
    "    cnn_dataset = ListsTestDataset(x, transform = test_transforms)\n",
    "    cnn_loader = torch.utils.data.DataLoader(cnn_dataset, batch_size = 32, shuffle = False)\n",
    "\n",
    "    predictions = []\n",
    "    for i, images in enumerate(cnn_loader):\n",
    "        images = Variable(images, requires_grad=False).cuda()\n",
    "        outputs = model(images).cuda()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        features = feature_extractor(images)\n",
    "        \n",
    "        total_features = torch.cat((total_features, features.detach().cpu()))\n",
    "        total_predicted = torch.cat((total_predicted, predicted.cpu().long()))\n",
    "        total_probabilities = torch.cat((total_probabilities,(torch.nn.Softmax()(outputs)).detach().cpu()))\n",
    "\n",
    "    return (total_features.numpy(), total_predicted.numpy(), total_probabilities.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimtsi/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "cnn_train_features, cnn_train_predictions, cnn_train_probabilities = get_cnn_features(feature_extractor_cnn, cnn, X_train_cnn)\n",
    "cnn_val_features, cnn_val_predictions, cnn_val_probabilities = get_cnn_features(feature_extractor_cnn, cnn, X_val_cnn)\n",
    "cnn_kaggle_features, cnn_kaggle_predictions, cnn_kaggle_probabilities = get_cnn_features(feature_extractor_cnn, cnn, kaggle_test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale and Preprocess for Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_handcrafted_train = scaler.fit_transform(train_handcrafted)\n",
    "scaled_handcrafted_val = scaler.fit_transform(test_mine_handcrafted)\n",
    "scaled_handcrafted_kaggle = scaler.fit_transform(kaggle_test_handcrafted_features)\n",
    "scaled_cnn_train_features = scaler.fit_transform(cnn_train_features)\n",
    "scaled_cnn_val_features = scaler.fit_transform(cnn_val_features)\n",
    "scaled_cnn_kaggle_features = scaler.fit_transform(cnn_kaggle_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Features DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = []\n",
    "\n",
    "for i in range(original_haralick.shape[1]):\n",
    "    feature_names.append(\"haralick\"+str(i))\n",
    "for i in range(original_moments.shape[1]):\n",
    "    feature_names.append(\"moments\"+str(i))\n",
    "for i in range(original_sizes.shape[1]):\n",
    "    feature_names.append(\"sizes\"+str(i))\n",
    "for i in range(scaled_cnn_train_features.shape[1]):\n",
    "    feature_names.append(\"deep\"+str(i))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##concat handcrafted and deep features\n",
    "\n",
    "NP_FEATURES_TRAIN = np.concatenate([scaled_handcrafted_train, scaled_cnn_train_features], axis = 1)\n",
    "NP_FEATURES_VAL = np.concatenate([scaled_handcrafted_val, scaled_cnn_val_features], axis = 1)\n",
    "NP_FEATURES_KAGGLE = np.concatenate([scaled_handcrafted_kaggle, scaled_cnn_kaggle_features], axis = 1)\n",
    "\n",
    "y_train = train_labels\n",
    "y_test = test_mine_labels\n",
    "\n",
    "X_train = pd.DataFrame(NP_FEATURES_TRAIN, columns = feature_names)\n",
    "X_test = pd.DataFrame(NP_FEATURES_VAL, columns = feature_names)\n",
    "X_kaggle = pd.DataFrame(NP_FEATURES_KAGGLE, columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=40)\n",
    "\n",
    "concatenated = np.concatenate([X_train, X_test], axis =0)\n",
    "concatenated = np.concatenate([concatenated, X_kaggle], axis =0)\n",
    "\n",
    "principalComponents = pca.fit_transform(concatenated)\n",
    "principalComponents.shape\n",
    "\n",
    "x_train = principalComponents[:len(X_train)]\n",
    "x_test = principalComponents[len(X_train):len(X_train)+len(X_test)]\n",
    "x_kaggle = principalComponents[len(X_train)+len(X_test):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Base Learners\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = x_train.shape[0]\n",
    "ntest = x_test.shape[0]\n",
    "nkaggle = x_kaggle.shape[0]\n",
    "\n",
    "SEED = 0 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "\n",
    "kf = KFold(n_splits = NFOLDS, random_state=SEED)\n",
    "\n",
    "# Class to extend the Sklearn classifier\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model, train_data, test_data, training_labels, test_labels):\n",
    "    y_pred_train = model.predict(train_data)\n",
    "    y_pred_test = model.predict(test_data)\n",
    "    print(\"Training Accuracy: \" +str(accuracy_score(training_labels, y_pred_train)))\n",
    "    print(\"Validation Accuracy: \" +str(accuracy_score(test_labels, y_pred_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Weak Classifiers Params\n",
    "\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 0.2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':500,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.2\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Support Vector Classifier parameters \n",
    "svc_params = {\n",
    "    'kernel' : 'rbf',\n",
    "    'C' : 0.025\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n",
      "Validation Accuracy: 0.6370940743220622\n",
      "elapsed time: 3.8123345375061035\n"
     ]
    }
   ],
   "source": [
    "##DTree\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "dt_model.fit(x_train, y_train)\n",
    "get_results(dt_model, x_train, x_test, y_train, y_test)\n",
    "\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"elapsed time: \"+str(elapsed_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6192962764140354\n",
      "Validation Accuracy: 0.5872112487445598\n",
      "elapsed time: 28.33855628967285\n"
     ]
    }
   ],
   "source": [
    "##RandomForest\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rf_model = RandomForestClassifier(**rf_params)\n",
    "rf_model.fit(x_train, y_train)\n",
    "get_results(rf_model, x_train, x_test, y_train, y_test)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"elapsed time: \"+str(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.897369576887414\n",
      "Validation Accuracy: 0.7656511550050218\n",
      "elapsed time: 327.04302167892456\n"
     ]
    }
   ],
   "source": [
    "##XGBoost\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_model = XGBClassifier(nthread=-1, learning_rate = 0.01, min_child_weight = 0.01, max_depth=5, )\n",
    "xgb_model.fit(x_train, y_train)\n",
    "get_results(xgb_model, x_train, x_test, y_train, y_test)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"elapsed time: \"+str(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.1382558196281294\n",
      "Validation Accuracy: 0.13491797790425175\n",
      "elapsed time: 195.90406155586243\n"
     ]
    }
   ],
   "source": [
    "##AdaBoost\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "ada_model = AdaBoostClassifier(**ada_params)\n",
    "ada_model.fit(x_train, y_train)\n",
    "get_results(ada_model, x_train, x_test, y_train, y_test)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"elapsed time: \"+str(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ExtraTrees\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "et_model = ExtraTreesClassifier(**svc_params)\n",
    "et_model.fit(x_train, y_train)\n",
    "get_results(et_model, x_train, x_test, y_train, y_test)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"elapsed time: \"+str(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SVM\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "svm = SVC(**ada_params)\n",
    "svm.fit(x_train, y_train)\n",
    "get_results(svm, x_train, x_test, y_train, y_test)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"elapsed time: \"+str(elapsed_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
