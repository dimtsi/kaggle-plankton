{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import math;\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchsummary import summary\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = pickle.load(open(\"pkl/train_images.pkl\", \"rb\"))\n",
    "train_labels = pickle.load(open(\"pkl/train_labels.pkl\", \"rb\"))\n",
    "train_filenames = pickle.load(open(\"pkl/train_filenames.pkl\", \"rb\"))\n",
    "test_images = pickle.load(open(\"pkl/test_images.pkl\", \"rb\"))\n",
    "test_filenames = pickle.load(open(\"pkl/test_filenames.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average width 73.32110394976037 , Average height: 66.46897207073211\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PIL\n",
    "\n",
    "widths, heights = [], [] \n",
    "sumx, sumy = 0, 0\n",
    "for i in train_images:\n",
    "    sumx += i.size[0]\n",
    "    widths.append(i.size[0])\n",
    "    sumy += i.size[1]\n",
    "    heights.append(i.size[1])\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.hist(widths)\n",
    "ax2.hist(heights, color = 'orange')\n",
    "fig.set_size_inches(12, 5)\n",
    "\n",
    "avg_width = np.mean(widths)\n",
    "avg_height = np.mean(heights)\n",
    "print('Average width {} , Average height: {}'.format(avg_width, avg_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListsTrainDataset(Dataset):\n",
    "    def __init__(self, list_of_images, list_of_labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            height (int): image height\n",
    "            width (int): image width\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "#         super().__init__()\n",
    "        self.data = list_of_images\n",
    "        self.labels = np.asarray(list_of_labels).reshape(-1,1)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        single_image = self.data[index]\n",
    "        single_image_label = self.labels[index]\n",
    "        # Transform image to tensor\n",
    "        if self.transform is not None:\n",
    "            img_as_tensor = self.transform(single_image)\n",
    "        # Return image and the label\n",
    "        return (img_as_tensor, single_image_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListsTestDataset(Dataset):\n",
    "    def __init__(self, list_of_images, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            height (int): image height\n",
    "            width (int): image width\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        self.data = list_of_images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        single_image = self.data[index]\n",
    "        if self.transform is not None:\n",
    "            img_as_tensor = self.transform(single_image)\n",
    "        # Return image ONLY\n",
    "        return img_as_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms and Dataset Creation\n",
    "def create_datasets_dataloaders(X_train, y_train, X_test= None, y_test = None):\n",
    "    test_transforms = transforms. Compose([\n",
    "        transforms.Resize(size=(64,64)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    train_transforms = transforms. Compose([\n",
    "        transforms.Resize(size=(64,64)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    train_dataset = ListsTrainDataset(X_train, y_train, transform = train_transforms)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 32, shuffle = True, num_workers=2)\n",
    "\n",
    "    if y_test is not None:\n",
    "        test_dataset = ListsTrainDataset(X_test, y_test, transform = test_transforms)\n",
    "    else:\n",
    "        test_dataset = ListsTestDataset(X_test, transform = test_transforms)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 100, shuffle = False)\n",
    "    return (train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import BasicBlock, Bottleneck, ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, num_epochs):\n",
    "    learning_rate = 0.0001\n",
    "    batch_size = data_loader.batch_size\n",
    "    criterion = nn.CrossEntropyLoss();\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate);\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate);\n",
    "    #Training\n",
    "    history = {'batch': [], 'loss': [], 'accuracy': []}\n",
    "    for epoch in range(num_epochs):\n",
    "            for i, (images, labels) in enumerate(data_loader):\n",
    "                images = Variable(images).cuda()\n",
    "                labels = Variable(labels).squeeze(1).long().cuda()#.cpu()\n",
    "                # Forward + Backward + Optimize\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _, argmax = torch.max(outputs, 1)\n",
    "                accuracy_train = (labels == argmax.squeeze()).float().mean()*100\n",
    "                # Show progress\n",
    "                if (i+1) % 32 == 0:\n",
    "                    log = \" \".join([\n",
    "                      \"Epoch : %d/%d\" % (epoch+1, num_epochs),\n",
    "                      \"Iter : %d/%d\" % (i+1, len(data_loader.dataset)//batch_size),\n",
    "                      \"Loss: %.4f\" % loss.item(),\n",
    "                      \"Accuracy: %.4f\" % accuracy_train])\n",
    "                    print('\\r{}'.format(log), end='')\n",
    "                    history['batch'].append(i)\n",
    "                    history['loss'].append(loss.item())\n",
    "                    history['accuracy'].append(accuracy_train.item())\n",
    "            print()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cnn.eval().cuda()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# for images, labels in test_loader:\n",
    "#     images = Variable(images)\n",
    "#     labels= labels.squeeze(1)\n",
    "#     outputs = cnn(images)\n",
    "#     _, predicted = torch.max(outputs.data, 1)\n",
    "#     total += labels.size(0)\n",
    "#     correct += (predicted.float() == labels).sum()\n",
    "# print('Test Accuracy of the model on the 60000 test images: %.4f %%' % (100*correct.item() / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def predict_test_set(model, loader, filenames):\n",
    "    predictions = []\n",
    "    for images in loader:\n",
    "        images = Variable(images).cuda()\n",
    "        outputs = model(images)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        predictions.extend(prediction.cpu().numpy())\n",
    "    results_df = pd.DataFrame({'image': test_filenames, 'class': predictions}, columns=['image', 'class'])\n",
    "    results_df.to_csv('results.csv',sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_loader, test_loader = create_datasets_dataloaders(train_images, train_labels)\n",
    "# cnn = ResNetMine(Bottleneck, [1, 1, 1, 1]).cuda()\n",
    "# trained_model = train(cnn, train_loader, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##predict on testset\n",
    "\n",
    "# test_transforms = transforms. Compose([\n",
    "#         transforms.CenterCrop(64),\n",
    "#         transforms.ToTensor()\n",
    "#     ])\n",
    "\n",
    "# test_dataset = ListsTestDataset(test_images, transform = test_transforms)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 100, shuffle = False)\n",
    "\n",
    "# predict_test_set(trained_model, test_loader, test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, test_loader, num_epochs):\n",
    "    learning_rate = 0.0001\n",
    "    batch_size = train_loader.batch_size\n",
    "    criterion = nn.CrossEntropyLoss();\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate);\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate);\n",
    "    #Training\n",
    "    history = {'batch': [], 'loss': [], 'accuracy': []}\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train().cuda()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = Variable(images).cuda()\n",
    "            labels = Variable(labels).squeeze(1).long().cuda()#.cpu()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, argmax = torch.max(outputs, 1)\n",
    "            accuracy_train = (labels == argmax.squeeze()).float().mean()*100\n",
    "            # Show progress\n",
    "            if (i+1) % 32 == 0:\n",
    "                log = \" \".join([\n",
    "                  \"Epoch : %d/%d\" % (epoch+1, num_epochs),\n",
    "                  \"Iter : %d/%d\" % (i+1, len(train_loader.dataset)//batch_size),\n",
    "                  \"Loss: %.4f\" % loss.item(),\n",
    "                  \"Accuracy: %.4f\" % accuracy_train])\n",
    "                print('\\r{}'.format(log), end='')\n",
    "                history['batch'].append(i)\n",
    "                history['loss'].append(loss.item())\n",
    "                history['accuracy'].append(accuracy_train.item())\n",
    "        print()\n",
    "        ##VALIDATION SCORE AFTER EVERY EPOCH\n",
    "        model.eval().cuda()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = Variable(images).cuda()\n",
    "            labels= labels.squeeze(1)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.cpu().long() == labels).sum()\n",
    "        print('VALIDATION SET ACCURACY: %.4f %%' % (100*correct.item() / total))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import NNs\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/100 Iter : 576/605 Loss: 2.0382 Accuracy: 40.6250\n",
      "Test Accuracy of the model on the 60000 test images: 9.0477 %\n",
      "Epoch : 2/100 Iter : 192/605 Loss: 2.4956 Accuracy: 31.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-33:\n",
      "Process Process-34:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8875c96b050a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNetMine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBasicBlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#     print(summary(cnn, (1,28,28)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mtrained_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-aaf50b22b74c>\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, test_loader, num_epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0maccuracy_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0margmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Show progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 110, in _worker_loop\n",
      "    data_queue.put((idx, samples))\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/multiprocessing/queues.py\", line 347, in put\n",
      "    self._writer.send_bytes(obj)\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(NNs)\n",
    "from NNs import *\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "trained_models = []\n",
    "for train_indexes, validation_indexes in kf.split(train_images):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    \n",
    "    for i in train_indexes:\n",
    "        X_train.append(train_images[i])\n",
    "        y_train.append(train_labels[i])\n",
    "    for j in validation_indexes:\n",
    "        X_val.append(train_images[j])\n",
    "        y_val.append(train_labels[j])\n",
    "    train_loader, test_loader = create_datasets_dataloaders(\n",
    "        X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    #Training\n",
    "    cnn = ResNetMine(BasicBlock, [1, 1, 1]).cuda()\n",
    "#     print(summary(cnn, (1,28,28)))\n",
    "    trained_model = train_and_validate(cnn, train_loader, test_loader, num_epochs=100)\n",
    "    trained_models.append(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (1024x2x2). Calculated output size: (1024x0x0). Output size is too small at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/THCUNN/generic/SpatialAveragePooling.cu:63",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b737d9cbfa3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNetMine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBottleneck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/NNs.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m#         x = self.layer4(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         return F.avg_pool2d(input, self.kernel_size, self.stride,\n\u001b[0;32m--> 547\u001b[0;31m                             self.padding, self.ceil_mode, self.count_include_pad)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (1024x2x2). Calculated output size: (1024x0x0). Output size is too small at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/THCUNN/generic/SpatialAveragePooling.cu:63"
     ]
    }
   ],
   "source": [
    "# cnn2 = trained_models[1]\n",
    "# cnn2.fc.weight\n",
    "\n",
    "cnn = ResNetMine(Bottleneck, [1, 1, 1]).cuda()\n",
    "summary(cnn, (1,28,28))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow - GPU",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
