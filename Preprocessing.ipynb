{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math;\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchsummary import summary\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = pickle.load(open(\"pkl/preprocessed_train_images.pkl\", \"rb\"))\n",
    "train_labels = pickle.load(open(\"pkl/train_labels.pkl\", \"rb\"))\n",
    "train_filenames = pickle.load(open(\"pkl/train_filenames.pkl\", \"rb\"))\n",
    "test_images = pickle.load(open(\"pkl/preprocessed_test_images.pkl\", \"rb\"))\n",
    "test_filenames = pickle.load(open(\"pkl/test_filenames.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average width 64.0 , Average height: 64.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PIL\n",
    "\n",
    "widths, heights = [], [] \n",
    "sumx, sumy = 0, 0\n",
    "for i in train_images:\n",
    "    sumx += i.size[0]\n",
    "    widths.append(i.size[0])\n",
    "    sumy += i.size[1]\n",
    "    heights.append(i.size[1])\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.hist(widths)\n",
    "ax2.hist(heights, color = 'orange')\n",
    "fig.set_size_inches(12, 5)\n",
    "\n",
    "avg_width = np.mean(widths)\n",
    "avg_height = np.mean(heights)\n",
    "print('Average width {} , Average height: {}'.format(avg_width, avg_height))\n",
    "\n",
    "norm_mean_width = np.mean(widths)\n",
    "norm_mean_height = np.mean(heights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CONVERT TO NUMPY TO CALCULATE MEAN,STD PER CHANNEL FOR NORMALIZATION \n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# np_train = []\n",
    "# np_test = []\n",
    "\n",
    "# for im in train_images:\n",
    "#     np_train.append(np.array(im))\n",
    "\n",
    "# for im in test_images:\n",
    "#     np_test.append(np.array(im))    \n",
    "    \n",
    "# arr = np.array(np_train) #len,x_pixels,y_pixels, channels\n",
    "# per_image_mean = np.mean(np_train, axis=(1,2)) #Shape (32,3)\n",
    "# per_image_std = np.std(np_train, axis=(1,2)) #Shape (32,3)\n",
    "\n",
    "# pop_channel_mean = np.mean(arr, axis=(0, 1, 2))/255\n",
    "# pop_channel_std = np.std(arr, axis=(0, 1, 2))/255\n",
    "\n",
    "# mean(array([0.70426004, 0.70426004, 0.70426004]),\n",
    "#  std array([0.43267642, 0.43267642, 0.43267642]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListsTrainDataset(Dataset):\n",
    "    def __init__(self, list_of_images, list_of_labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            height (int): image height\n",
    "            width (int): image width\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "#         super().__init__()\n",
    "        self.data = list_of_images\n",
    "        self.labels = np.asarray(list_of_labels).reshape(-1,1)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        single_image = self.data[index]\n",
    "        single_image_label = self.labels[index]\n",
    "        # Transform image to tensor\n",
    "        if self.transform is not None:\n",
    "            img_as_tensor = self.transform(single_image)\n",
    "        # Return image and the label\n",
    "        return (img_as_tensor, single_image_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListsTestDataset(Dataset):\n",
    "    def __init__(self, list_of_images, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            height (int): image height\n",
    "            width (int): image width\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        self.data = list_of_images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        single_image = self.data[index]\n",
    "        if self.transform is not None:\n",
    "            img_as_tensor = self.transform(single_image)\n",
    "        # Return image ONLY\n",
    "        return img_as_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms and Dataset Creation\n",
    "def create_datasets_dataloaders(X_train, y_train, X_test= None, y_test = None, batch_size = 32):\n",
    "    test_transforms = transforms. Compose([\n",
    "#         transforms.CenterCrop(64),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.70426004, 0.70426004, 0.70426004],\n",
    "                    std =[0.43267642, 0.43267642, 0.43267642])\n",
    "    ])\n",
    "    \n",
    "    train_transforms = transforms. Compose([\n",
    "#         transforms.CenterCrop(64),\n",
    "        transforms.Grayscale(),\n",
    "#         transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=180),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.70426004, 0.70426004, 0.70426004],\n",
    "                            std =[0.43267642, 0.43267642, 0.43267642])\n",
    "    ])\n",
    "    \n",
    "    train_dataset = ListsTrainDataset(X_train, y_train, transform = train_transforms)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers=0)\n",
    "\n",
    "    if y_test is not None:\n",
    "        test_dataset = ListsTrainDataset(X_test, y_test, transform = test_transforms)\n",
    "    else:\n",
    "        test_dataset = ListsTestDataset(X_test, transform = test_transforms)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\n",
    "    return (train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import BasicBlock, Bottleneck, ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, num_epochs):\n",
    "    learning_rate = 0.0005\n",
    "    batch_size = data_loader.batch_size\n",
    "    criterion = nn.CrossEntropyLoss();\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate);\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate);\n",
    "    #Training\n",
    "    history = {'batch': [], 'loss': [], 'accuracy': []}\n",
    "    for epoch in range(num_epochs):\n",
    "            for i, (images, labels) in enumerate(data_loader):\n",
    "                images = Variable(images).cuda()\n",
    "                labels = Variable(labels).squeeze(1).long().cuda()#.cpu()\n",
    "                # Forward + Backward + Optimize\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _, argmax = torch.max(outputs, 1)\n",
    "                accuracy_train = (labels == argmax.squeeze()).float().mean()*100\n",
    "                # Show progress\n",
    "                if (i+1) % 32 == 0:\n",
    "                    log = \" \".join([\n",
    "                      \"Epoch : %d/%d\" % (epoch+1, num_epochs),\n",
    "                      \"Iter : %d/%d\" % (i+1, len(data_loader.dataset)//batch_size),\n",
    "                      \"Loss: %.4f\" % loss.item(),\n",
    "                      \"Accuracy: %.4f\" % accuracy_train])\n",
    "                    print('\\r{}'.format(log), end='')\n",
    "                    history['batch'].append(i)\n",
    "                    history['loss'].append(loss.item())\n",
    "                    history['accuracy'].append(accuracy_train.item())\n",
    "            print()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cnn.eval().cuda()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# for images, labels in test_loader:\n",
    "#     images = Variable(images)\n",
    "#     labels= labels.squeeze(1)\n",
    "#     outputs = cnn(images)\n",
    "#     _, predicted = torch.max(outputs.data, 1)\n",
    "#     total += labels.size(0)\n",
    "#     correct += (predicted.float() == labels).sum()\n",
    "# print('Test Accuracy of the model on the 60000 test images: %.4f %%' % (100*correct.item() / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def predict_test_set(model, loader, filenames):\n",
    "    predictions = []\n",
    "    for images in loader:\n",
    "        images = Variable(images).cuda()\n",
    "        outputs = model(images)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        predictions.extend(prediction.cpu().numpy())\n",
    "    results_df = pd.DataFrame({'image': test_filenames, 'class': predictions}, columns=['image', 'class'])\n",
    "    results_df.to_csv('results.csv',sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_loader, test_loader = create_datasets_dataloaders(train_images, train_labels)\n",
    "# cnn = ResNetMine(Bottleneck, [1, 1, 1, 1]).cuda()\n",
    "# trained_model = train(cnn, train_loader, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##predict on testset\n",
    "\n",
    "# test_transforms = transforms. Compose([\n",
    "#         transforms.CenterCrop(64),\n",
    "#         transforms.ToTensor()\n",
    "#     ])\n",
    "\n",
    "# test_dataset = ListsTestDataset(test_images, transform = test_transforms)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 100, shuffle = False)\n",
    "\n",
    "# predict_test_set(trained_model, test_loader, test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, test_loader, num_epochs):\n",
    "    learning_rate = 0.001\n",
    "    weight_decay = 0\n",
    "    batch_size = train_loader.batch_size\n",
    "    criterion = nn.CrossEntropyLoss();\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay);\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, 'max', factor=0.1, patience=5, verbose=True)\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate);\n",
    "    #Training\n",
    "    history = {'batch': [], 'loss': [], 'accuracy': []}\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train().cuda()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = Variable(images).cuda()\n",
    "            labels = Variable(labels).squeeze(1).long().cuda()#.cpu()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, argmax = torch.max(outputs, 1)\n",
    "            accuracy_train = (labels == argmax.squeeze()).float().mean()*100\n",
    "            # Show progress\n",
    "            if (i+1) % 32 == 0:\n",
    "                log = \" \".join([\n",
    "                  \"Epoch : %d/%d\" % (epoch+1, num_epochs),\n",
    "                  \"Iter : %d/%d\" % (i+1, len(train_loader.dataset)//batch_size),\n",
    "                  \"Loss: %.4f\" % loss.item(),\n",
    "                  \"Accuracy: %.4f\" % accuracy_train])\n",
    "                print('\\r{}'.format(log), end='')\n",
    "                history['batch'].append(i)\n",
    "                history['loss'].append(loss.item())\n",
    "                history['accuracy'].append(accuracy_train.item())\n",
    "        print()\n",
    "        ##VALIDATION SCORE AFTER EVERY EPOCH\n",
    "        model.eval().cuda()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = Variable(images).cuda()\n",
    "            labels= labels.squeeze(1)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.cpu().long() == labels).sum()\n",
    "        print('VALIDATION SET ACCURACY: %.4f %%' % (100*correct.item() / total))\n",
    "        scheduler.step(correct.item() / total)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]             576\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "              ReLU-3           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-4           [-1, 64, 16, 16]               0\n",
      "            Conv2d-5           [-1, 64, 16, 16]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 16, 16]             128\n",
      "              ReLU-7           [-1, 64, 16, 16]               0\n",
      "            Conv2d-8           [-1, 64, 16, 16]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
      "             ReLU-10           [-1, 64, 16, 16]               0\n",
      "           Conv2d-11          [-1, 256, 16, 16]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 16, 16]             512\n",
      "           Conv2d-13          [-1, 256, 16, 16]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 16, 16]             512\n",
      "             ReLU-15          [-1, 256, 16, 16]               0\n",
      "       Bottleneck-16          [-1, 256, 16, 16]               0\n",
      "           Conv2d-17           [-1, 64, 16, 16]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 16, 16]             128\n",
      "             ReLU-19           [-1, 64, 16, 16]               0\n",
      "           Conv2d-20           [-1, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 16, 16]             128\n",
      "             ReLU-22           [-1, 64, 16, 16]               0\n",
      "           Conv2d-23          [-1, 256, 16, 16]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 16, 16]             512\n",
      "             ReLU-25          [-1, 256, 16, 16]               0\n",
      "       Bottleneck-26          [-1, 256, 16, 16]               0\n",
      "           Conv2d-27           [-1, 64, 16, 16]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 16, 16]             128\n",
      "             ReLU-29           [-1, 64, 16, 16]               0\n",
      "           Conv2d-30           [-1, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 16, 16]             128\n",
      "             ReLU-32           [-1, 64, 16, 16]               0\n",
      "           Conv2d-33          [-1, 256, 16, 16]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 16, 16]             512\n",
      "             ReLU-35          [-1, 256, 16, 16]               0\n",
      "       Bottleneck-36          [-1, 256, 16, 16]               0\n",
      "           Conv2d-37          [-1, 128, 16, 16]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 16, 16]             256\n",
      "             ReLU-39          [-1, 128, 16, 16]               0\n",
      "           Conv2d-40            [-1, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-41            [-1, 128, 8, 8]             256\n",
      "             ReLU-42            [-1, 128, 8, 8]               0\n",
      "           Conv2d-43            [-1, 512, 8, 8]          65,536\n",
      "      BatchNorm2d-44            [-1, 512, 8, 8]           1,024\n",
      "           Conv2d-45            [-1, 512, 8, 8]         131,072\n",
      "      BatchNorm2d-46            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-47            [-1, 512, 8, 8]               0\n",
      "       Bottleneck-48            [-1, 512, 8, 8]               0\n",
      "           Conv2d-49            [-1, 128, 8, 8]          65,536\n",
      "      BatchNorm2d-50            [-1, 128, 8, 8]             256\n",
      "             ReLU-51            [-1, 128, 8, 8]               0\n",
      "           Conv2d-52            [-1, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-53            [-1, 128, 8, 8]             256\n",
      "             ReLU-54            [-1, 128, 8, 8]               0\n",
      "           Conv2d-55            [-1, 512, 8, 8]          65,536\n",
      "      BatchNorm2d-56            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-57            [-1, 512, 8, 8]               0\n",
      "       Bottleneck-58            [-1, 512, 8, 8]               0\n",
      "           Conv2d-59            [-1, 128, 8, 8]          65,536\n",
      "      BatchNorm2d-60            [-1, 128, 8, 8]             256\n",
      "             ReLU-61            [-1, 128, 8, 8]               0\n",
      "           Conv2d-62            [-1, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-63            [-1, 128, 8, 8]             256\n",
      "             ReLU-64            [-1, 128, 8, 8]               0\n",
      "           Conv2d-65            [-1, 512, 8, 8]          65,536\n",
      "      BatchNorm2d-66            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-67            [-1, 512, 8, 8]               0\n",
      "       Bottleneck-68            [-1, 512, 8, 8]               0\n",
      "           Conv2d-69            [-1, 128, 8, 8]          65,536\n",
      "      BatchNorm2d-70            [-1, 128, 8, 8]             256\n",
      "             ReLU-71            [-1, 128, 8, 8]               0\n",
      "           Conv2d-72            [-1, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-73            [-1, 128, 8, 8]             256\n",
      "             ReLU-74            [-1, 128, 8, 8]               0\n",
      "           Conv2d-75            [-1, 512, 8, 8]          65,536\n",
      "      BatchNorm2d-76            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-77            [-1, 512, 8, 8]               0\n",
      "       Bottleneck-78            [-1, 512, 8, 8]               0\n",
      "           Conv2d-79            [-1, 128, 8, 8]          65,536\n",
      "      BatchNorm2d-80            [-1, 128, 8, 8]             256\n",
      "             ReLU-81            [-1, 128, 8, 8]               0\n",
      "           Conv2d-82            [-1, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-83            [-1, 128, 8, 8]             256\n",
      "             ReLU-84            [-1, 128, 8, 8]               0\n",
      "           Conv2d-85            [-1, 512, 8, 8]          65,536\n",
      "      BatchNorm2d-86            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-87            [-1, 512, 8, 8]               0\n",
      "       Bottleneck-88            [-1, 512, 8, 8]               0\n",
      "           Conv2d-89            [-1, 128, 8, 8]          65,536\n",
      "      BatchNorm2d-90            [-1, 128, 8, 8]             256\n",
      "             ReLU-91            [-1, 128, 8, 8]               0\n",
      "           Conv2d-92            [-1, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-93            [-1, 128, 8, 8]             256\n",
      "             ReLU-94            [-1, 128, 8, 8]               0\n",
      "           Conv2d-95            [-1, 512, 8, 8]          65,536\n",
      "      BatchNorm2d-96            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-97            [-1, 512, 8, 8]               0\n",
      "       Bottleneck-98            [-1, 512, 8, 8]               0\n",
      "           Conv2d-99            [-1, 128, 8, 8]          65,536\n",
      "     BatchNorm2d-100            [-1, 128, 8, 8]             256\n",
      "            ReLU-101            [-1, 128, 8, 8]               0\n",
      "          Conv2d-102            [-1, 128, 8, 8]         147,456\n",
      "     BatchNorm2d-103            [-1, 128, 8, 8]             256\n",
      "            ReLU-104            [-1, 128, 8, 8]               0\n",
      "          Conv2d-105            [-1, 512, 8, 8]          65,536\n",
      "     BatchNorm2d-106            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-107            [-1, 512, 8, 8]               0\n",
      "      Bottleneck-108            [-1, 512, 8, 8]               0\n",
      "          Conv2d-109            [-1, 128, 8, 8]          65,536\n",
      "     BatchNorm2d-110            [-1, 128, 8, 8]             256\n",
      "            ReLU-111            [-1, 128, 8, 8]               0\n",
      "          Conv2d-112            [-1, 128, 8, 8]         147,456\n",
      "     BatchNorm2d-113            [-1, 128, 8, 8]             256\n",
      "            ReLU-114            [-1, 128, 8, 8]               0\n",
      "          Conv2d-115            [-1, 512, 8, 8]          65,536\n",
      "     BatchNorm2d-116            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-117            [-1, 512, 8, 8]               0\n",
      "      Bottleneck-118            [-1, 512, 8, 8]               0\n",
      "AdaptiveAvgPool2d-119            [-1, 512, 1, 1]               0\n",
      "          Linear-120                  [-1, 256]         131,328\n",
      "            ReLU-121                  [-1, 256]               0\n",
      "         Dropout-122                  [-1, 256]               0\n",
      "          Linear-123                  [-1, 121]          31,097\n",
      "================================================================\n",
      "Total params: 2,718,777\n",
      "Trainable params: 2,718,777\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 22.95\n",
      "Params size (MB): 10.37\n",
      "Estimated Total Size (MB): 33.34\n",
      "----------------------------------------------------------------\n",
      "Epoch : 1/150 Iter : 672/693 Loss: 3.0469 Accuracy: 15.6250\n",
      "VALIDATION SET ACCURACY: 30.2925 %\n",
      "Epoch : 2/150 Iter : 672/693 Loss: 2.6774 Accuracy: 31.2500\n",
      "VALIDATION SET ACCURACY: 31.9286 %\n",
      "Epoch : 3/150 Iter : 672/693 Loss: 2.2714 Accuracy: 37.5000\n",
      "VALIDATION SET ACCURACY: 23.0540 %\n",
      "Epoch : 4/150 Iter : 672/693 Loss: 2.5219 Accuracy: 34.3750\n",
      "VALIDATION SET ACCURACY: 44.4224 %\n",
      "Epoch : 5/150 Iter : 672/693 Loss: 2.2601 Accuracy: 43.7500\n",
      "VALIDATION SET ACCURACY: 32.1269 %\n",
      "Epoch : 6/150 Iter : 672/693 Loss: 2.6689 Accuracy: 34.3750\n",
      "VALIDATION SET ACCURACY: 45.6123 %\n",
      "Epoch : 7/150 Iter : 672/693 Loss: 2.1850 Accuracy: 43.7500\n",
      "VALIDATION SET ACCURACY: 35.3495 %\n",
      "Epoch : 8/150 Iter : 672/693 Loss: 2.5945 Accuracy: 28.1250\n",
      "VALIDATION SET ACCURACY: 45.3644 %\n",
      "Epoch : 9/150 Iter : 672/693 Loss: 2.0811 Accuracy: 40.6250\n",
      "VALIDATION SET ACCURACY: 46.8022 %\n",
      "Epoch : 10/150 Iter : 672/693 Loss: 2.3112 Accuracy: 46.8750\n",
      "VALIDATION SET ACCURACY: 50.3223 %\n",
      "Epoch : 11/150 Iter : 672/693 Loss: 1.8280 Accuracy: 46.8750\n",
      "VALIDATION SET ACCURACY: 50.3718 %\n",
      "Epoch : 12/150 Iter : 672/693 Loss: 1.7118 Accuracy: 56.2500\n",
      "VALIDATION SET ACCURACY: 47.8433 %\n",
      "Epoch : 13/150 Iter : 672/693 Loss: 1.4159 Accuracy: 68.7500\n",
      "VALIDATION SET ACCURACY: 54.0407 %\n",
      "Epoch : 14/150 Iter : 672/693 Loss: 1.6193 Accuracy: 40.6250\n",
      "VALIDATION SET ACCURACY: 56.0734 %\n",
      "Epoch : 15/150 Iter : 672/693 Loss: 1.8220 Accuracy: 40.6250\n",
      "VALIDATION SET ACCURACY: 55.2305 %\n",
      "Epoch : 16/150 Iter : 672/693 Loss: 1.5910 Accuracy: 34.3750\n",
      "VALIDATION SET ACCURACY: 53.6936 %\n",
      "Epoch : 17/150 Iter : 672/693 Loss: 1.9866 Accuracy: 46.8750\n",
      "VALIDATION SET ACCURACY: 51.3138 %\n",
      "Epoch : 18/150 Iter : 672/693 Loss: 1.6435 Accuracy: 53.1250\n",
      "VALIDATION SET ACCURACY: 60.1884 %\n",
      "Epoch : 19/150 Iter : 672/693 Loss: 1.3978 Accuracy: 56.2500\n",
      "VALIDATION SET ACCURACY: 58.4036 %\n",
      "Epoch : 20/150 Iter : 512/693 Loss: 1.5560 Accuracy: 46.8750"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import NNs\n",
    "import math\n",
    "importlib.reload(NNs)\n",
    "from NNs import *\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=12, random_state=None, shuffle=True)\n",
    "trained_models = []\n",
    "for train_indexes, validation_indexes in kf.split(train_images):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    \n",
    "    for i in train_indexes:\n",
    "        X_train.append(train_images[i])\n",
    "        y_train.append(train_labels[i])\n",
    "    for j in validation_indexes:\n",
    "        X_val.append(train_images[j])\n",
    "        y_val.append(train_labels[j])\n",
    "    train_loader, test_loader = create_datasets_dataloaders(\n",
    "        X_train, y_train, X_val, y_val, batch_size = 32)\n",
    "    \n",
    "    #Training\n",
    "    cnn = ResNetMine(Bottleneck, [3, 8, 36, 3]).cuda()\n",
    "#     cnn = CNN().cuda()\n",
    "    summary(cnn, (1,64,64))\n",
    "\n",
    "#     print(summary(cnn, (1,28,28)))\n",
    "    trained_model = train_and_validate(cnn, train_loader, test_loader, num_epochs=150)\n",
    "    trained_models.append(trained_model)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = trained_models[0].eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on testset\n",
    "def predict_test_set(model, loader, filenames):\n",
    "    predictions = []\n",
    "    for images in loader:\n",
    "        images = Variable(images).cuda()\n",
    "        outputs = model(images)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        predictions.extend(prediction.cpu().numpy())\n",
    "    results_df = pd.DataFrame({'image': test_filenames, 'class': predictions}, columns=['image', 'class'])\n",
    "    results_df.to_csv('results.csv',sep = ',', index = False)\n",
    "\n",
    "\n",
    "test_transforms = transforms. Compose([\n",
    "        transforms.CenterCrop(64),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "test_dataset = ListsTestDataset(test_images, transform = test_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 100, shuffle = False)\n",
    "\n",
    "predict_test_set(final_model, test_loader, test_filenames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow - GPU",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
