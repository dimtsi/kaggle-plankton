{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import math;\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchsummary import summary\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = pickle.load(open(\"pkl/train_images.pkl\", \"rb\"))\n",
    "train_labels = pickle.load(open(\"pkl/train_labels.pkl\", \"rb\"))\n",
    "train_filenames = pickle.load(open(\"pkl/train_filenames.pkl\", \"rb\"))\n",
    "test_images = pickle.load(open(\"pkl/test_images.pkl\", \"rb\"))\n",
    "test_filenames = pickle.load(open(\"pkl/test_filenames.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average width 73.32110394976037 , Average height: 66.46897207073211\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PIL\n",
    "\n",
    "widths, heights = [], [] \n",
    "sumx, sumy = 0, 0\n",
    "for i in train_images:\n",
    "    sumx += i.size[0]\n",
    "    widths.append(i.size[0])\n",
    "    sumy += i.size[1]\n",
    "    heights.append(i.size[1])\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.hist(widths)\n",
    "ax2.hist(heights, color = 'orange')\n",
    "fig.set_size_inches(12, 5)\n",
    "\n",
    "avg_width = np.mean(widths)\n",
    "avg_height = np.mean(heights)\n",
    "print('Average width {} , Average height: {}'.format(avg_width, avg_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListsTrainDataset(Dataset):\n",
    "    def __init__(self, list_of_images, list_of_labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            height (int): image height\n",
    "            width (int): image width\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "#         super().__init__()\n",
    "        self.data = list_of_images\n",
    "        self.labels = np.asarray(list_of_labels).reshape(-1,1)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        single_image = self.data[index]\n",
    "        single_image_label = self.labels[index]\n",
    "        # Transform image to tensor\n",
    "        if self.transform is not None:\n",
    "            img_as_tensor = self.transform(single_image)\n",
    "        # Return image and the label\n",
    "        return (img_as_tensor, single_image_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListsTestDataset(Dataset):\n",
    "    def __init__(self, list_of_images, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            height (int): image height\n",
    "            width (int): image width\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        self.data = list_of_images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        single_image = self.data[index]\n",
    "        if self.transform is not None:\n",
    "            img_as_tensor = self.transform(single_image)\n",
    "        # Return image ONLY\n",
    "        return img_as_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms and Dataset Creation\n",
    "def create_datasets_dataloaders(X_train, y_train, X_test= None, y_test = None):\n",
    "    test_transforms = transforms. Compose([\n",
    "        transforms.CenterCrop(64),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    train_transforms = transforms. Compose([\n",
    "        transforms.CenterCrop(64),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    train_dataset = ListsTrainDataset(X_train, y_train, transform = train_transforms)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 32, shuffle = True, num_workers=2)\n",
    "\n",
    "    if y_test is not None:\n",
    "        test_dataset = ListsTrainDataset(X_test, y_test, transform = test_transforms)\n",
    "    else:\n",
    "        test_dataset = ListsTestDataset(X_test, transform = test_transforms)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 100, shuffle = False)\n",
    "    return (train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import BasicBlock, Bottleneck, ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, num_epochs):\n",
    "    learning_rate = 0.0001\n",
    "    batch_size = data_loader.batch_size\n",
    "    criterion = nn.CrossEntropyLoss();\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate);\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate);\n",
    "    #Training\n",
    "    history = {'batch': [], 'loss': [], 'accuracy': []}\n",
    "    for epoch in range(num_epochs):\n",
    "            for i, (images, labels) in enumerate(data_loader):\n",
    "                images = Variable(images).cuda()\n",
    "                labels = Variable(labels).squeeze(1).long().cuda()#.cpu()\n",
    "                # Forward + Backward + Optimize\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _, argmax = torch.max(outputs, 1)\n",
    "                accuracy_train = (labels == argmax.squeeze()).float().mean()*100\n",
    "                # Show progress\n",
    "                if (i+1) % 32 == 0:\n",
    "                    log = \" \".join([\n",
    "                      \"Epoch : %d/%d\" % (epoch+1, num_epochs),\n",
    "                      \"Iter : %d/%d\" % (i+1, len(data_loader.dataset)//batch_size),\n",
    "                      \"Loss: %.4f\" % loss.item(),\n",
    "                      \"Accuracy: %.4f\" % accuracy_train])\n",
    "                    print('\\r{}'.format(log), end='')\n",
    "                    history['batch'].append(i)\n",
    "                    history['loss'].append(loss.item())\n",
    "                    history['accuracy'].append(accuracy_train.item())\n",
    "            print()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cnn.eval().cuda()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# for images, labels in test_loader:\n",
    "#     images = Variable(images)\n",
    "#     labels= labels.squeeze(1)\n",
    "#     outputs = cnn(images)\n",
    "#     _, predicted = torch.max(outputs.data, 1)\n",
    "#     total += labels.size(0)\n",
    "#     correct += (predicted.float() == labels).sum()\n",
    "# print('Test Accuracy of the model on the 60000 test images: %.4f %%' % (100*correct.item() / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def predict_test_set(model, loader, filenames):\n",
    "    predictions = []\n",
    "    for images in loader:\n",
    "        images = Variable(images).cuda()\n",
    "        outputs = model(images)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        predictions.extend(prediction.cpu().numpy())\n",
    "    results_df = pd.DataFrame({'image': test_filenames, 'class': predictions}, columns=['image', 'class'])\n",
    "    results_df.to_csv('results.csv',sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_loader, test_loader = create_datasets_dataloaders(train_images, train_labels)\n",
    "# cnn = ResNetMine(Bottleneck, [1, 1, 1, 1]).cuda()\n",
    "# trained_model = train(cnn, train_loader, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##predict on testset\n",
    "\n",
    "# test_transforms = transforms. Compose([\n",
    "#         transforms.CenterCrop(64),\n",
    "#         transforms.ToTensor()\n",
    "#     ])\n",
    "\n",
    "# test_dataset = ListsTestDataset(test_images, transform = test_transforms)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 100, shuffle = False)\n",
    "\n",
    "# predict_test_set(trained_model, test_loader, test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, test_loader, num_epochs):\n",
    "    learning_rate = 0.0001\n",
    "    batch_size = train_loader.batch_size\n",
    "    criterion = nn.CrossEntropyLoss();\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate);\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate);\n",
    "    #Training\n",
    "    history = {'batch': [], 'loss': [], 'accuracy': []}\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train().cuda()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = Variable(images).cuda()\n",
    "            labels = Variable(labels).squeeze(1).long().cuda()#.cpu()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, argmax = torch.max(outputs, 1)\n",
    "            accuracy_train = (labels == argmax.squeeze()).float().mean()*100\n",
    "            # Show progress\n",
    "            if (i+1) % 32 == 0:\n",
    "                log = \" \".join([\n",
    "                  \"Epoch : %d/%d\" % (epoch+1, num_epochs),\n",
    "                  \"Iter : %d/%d\" % (i+1, len(train_loader.dataset)//batch_size),\n",
    "                  \"Loss: %.4f\" % loss.item(),\n",
    "                  \"Accuracy: %.4f\" % accuracy_train])\n",
    "                print('\\r{}'.format(log), end='')\n",
    "                history['batch'].append(i)\n",
    "                history['loss'].append(loss.item())\n",
    "                history['accuracy'].append(accuracy_train.item())\n",
    "        print()\n",
    "        ##VALIDATION SCORE AFTER EVERY EPOCH\n",
    "        model.eval().cuda()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = Variable(images).cuda()\n",
    "            labels= labels.squeeze(1)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.cpu().long() == labels).sum()\n",
    "        print('Test Accuracy of the model on the 60000 test images: %.4f %%' % (100*correct.item() / total))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import NNs\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'NNs' from '/home/dimtsi/Dropbox/UvA/1st Semester/Applied Machine Learning/Project/NNs.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(NNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f59a5320278>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/multiprocessing/reduction.py\", line 182, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/home/dimtsi/anaconda3/envs/tensorflow/lib/python3.6/multiprocessing/reduction.py\", line 153, in recvfds\n",
      "    msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_LEN(bytes_size))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [32 x 9216], m2: [1024 x 121] at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/THC/generic/THCTensorMathBlas.cu:249",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-375ee8308fcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNetMine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBottleneck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#     print(summary(cnn, (1,28,28)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mtrained_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-aaf50b22b74c>\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, test_loader, num_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# Forward + Backward + Optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/UvA/1st Semester/Applied Machine Learning/Project/NNs.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [32 x 9216], m2: [1024 x 121] at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/THC/generic/THCTensorMathBlas.cu:249"
     ]
    }
   ],
   "source": [
    "from NNs import *\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "trained_models = []\n",
    "for train_indexes, validation_indexes in kf.split(train_images):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    \n",
    "    for i in train_indexes:\n",
    "        X_train.append(train_images[i])\n",
    "        y_train.append(train_labels[i])\n",
    "    for j in validation_indexes:\n",
    "        X_val.append(train_images[j])\n",
    "        y_val.append(train_labels[j])\n",
    "    train_loader, test_loader = create_datasets_dataloaders(\n",
    "        X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    #Training\n",
    "    cnn = ResNetMine(Bottleneck, [1, 1, 1]).cuda()\n",
    "#     print(summary(cnn, (1,28,28)))\n",
    "    trained_model = train_and_validate(cnn, train_loader, test_loader, num_epochs=100)\n",
    "    trained_models.append(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (1024x2x2). Calculated output size: (1024x0x0). Output size is too small at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/THCUNN/generic/SpatialAveragePooling.cu:63",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-b737d9cbfa3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNetMine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBottleneck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-be9ce895bb57>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#         x = self.layer4(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         return F.avg_pool2d(input, self.kernel_size, self.stride,\n\u001b[0;32m--> 547\u001b[0;31m                             self.padding, self.ceil_mode, self.count_include_pad)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (1024x2x2). Calculated output size: (1024x0x0). Output size is too small at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/THCUNN/generic/SpatialAveragePooling.cu:63"
     ]
    }
   ],
   "source": [
    "# cnn2 = trained_models[1]\n",
    "# cnn2.fc.weight\n",
    "\n",
    "cnn = ResNetMine(Bottleneck, [1, 1, 1]).cuda()\n",
    "summary(cnn, (1,28,28))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-GPU",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
