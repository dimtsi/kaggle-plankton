{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import math;\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchsummary import summary\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = pickle.load(open(\"pkl/preprocessed_train_images.pkl\", \"rb\"))\n",
    "train_labels = pickle.load(open(\"pkl/train_labels.pkl\", \"rb\"))\n",
    "train_filenames = pickle.load(open(\"pkl/train_filenames.pkl\", \"rb\"))\n",
    "test_images = pickle.load(open(\"pkl/preprocessed_test_images.pkl\", \"rb\"))\n",
    "test_filenames = pickle.load(open(\"pkl/test_filenames.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average width 64.0 , Average height: 64.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PIL\n",
    "\n",
    "widths, heights = [], [] \n",
    "sumx, sumy = 0, 0\n",
    "for i in train_images:\n",
    "    sumx += i.size[0]\n",
    "    widths.append(i.size[0])\n",
    "    sumy += i.size[1]\n",
    "    heights.append(i.size[1])\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.hist(widths)\n",
    "ax2.hist(heights, color = 'orange')\n",
    "fig.set_size_inches(12, 5)\n",
    "\n",
    "avg_width = np.mean(widths)\n",
    "avg_height = np.mean(heights)\n",
    "print('Average width {} , Average height: {}'.format(avg_width, avg_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListsTrainDataset(Dataset):\n",
    "    def __init__(self, list_of_images, list_of_labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            height (int): image height\n",
    "            width (int): image width\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "#         super().__init__()\n",
    "        self.data = list_of_images\n",
    "        self.labels = np.asarray(list_of_labels).reshape(-1,1)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        single_image = self.data[index]\n",
    "        single_image_label = self.labels[index]\n",
    "        # Transform image to tensor\n",
    "        if self.transform is not None:\n",
    "            img_as_tensor = self.transform(single_image)\n",
    "        # Return image and the label\n",
    "        return (img_as_tensor, single_image_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListsTestDataset(Dataset):\n",
    "    def __init__(self, list_of_images, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            height (int): image height\n",
    "            width (int): image width\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        self.data = list_of_images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        single_image = self.data[index]\n",
    "        if self.transform is not None:\n",
    "            img_as_tensor = self.transform(single_image)\n",
    "        # Return image ONLY\n",
    "        return img_as_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms and Dataset Creation\n",
    "def create_datasets_dataloaders(X_train, y_train, X_test= None, y_test = None):\n",
    "    test_transforms = transforms. Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    train_transforms = transforms. Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=360),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    train_dataset = ListsTrainDataset(X_train, y_train, transform = train_transforms)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 32, shuffle = True, num_workers=0)\n",
    "\n",
    "    if y_test is not None:\n",
    "        test_dataset = ListsTrainDataset(X_test, y_test, transform = test_transforms)\n",
    "    else:\n",
    "        test_dataset = ListsTestDataset(X_test, transform = test_transforms)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 32, shuffle = False)\n",
    "    return (train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import BasicBlock, Bottleneck, ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, num_epochs):\n",
    "    learning_rate = 0.0005\n",
    "    batch_size = data_loader.batch_size\n",
    "    criterion = nn.CrossEntropyLoss();\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate);\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate);\n",
    "    #Training\n",
    "    history = {'batch': [], 'loss': [], 'accuracy': []}\n",
    "    for epoch in range(num_epochs):\n",
    "            for i, (images, labels) in enumerate(data_loader):\n",
    "                images = Variable(images).cuda()\n",
    "                labels = Variable(labels).squeeze(1).long().cuda()#.cpu()\n",
    "                # Forward + Backward + Optimize\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _, argmax = torch.max(outputs, 1)\n",
    "                accuracy_train = (labels == argmax.squeeze()).float().mean()*100\n",
    "                # Show progress\n",
    "                if (i+1) % 32 == 0:\n",
    "                    log = \" \".join([\n",
    "                      \"Epoch : %d/%d\" % (epoch+1, num_epochs),\n",
    "                      \"Iter : %d/%d\" % (i+1, len(data_loader.dataset)//batch_size),\n",
    "                      \"Loss: %.4f\" % loss.item(),\n",
    "                      \"Accuracy: %.4f\" % accuracy_train])\n",
    "                    print('\\r{}'.format(log), end='')\n",
    "                    history['batch'].append(i)\n",
    "                    history['loss'].append(loss.item())\n",
    "                    history['accuracy'].append(accuracy_train.item())\n",
    "            print()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cnn.eval().cuda()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# for images, labels in test_loader:\n",
    "#     images = Variable(images)\n",
    "#     labels= labels.squeeze(1)\n",
    "#     outputs = cnn(images)\n",
    "#     _, predicted = torch.max(outputs.data, 1)\n",
    "#     total += labels.size(0)\n",
    "#     correct += (predicted.float() == labels).sum()\n",
    "# print('Test Accuracy of the model on the 60000 test images: %.4f %%' % (100*correct.item() / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def predict_test_set(model, loader, filenames):\n",
    "    predictions = []\n",
    "    for images in loader:\n",
    "        images = Variable(images).cuda()\n",
    "        outputs = model(images)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        predictions.extend(prediction.cpu().numpy())\n",
    "    results_df = pd.DataFrame({'image': test_filenames, 'class': predictions}, columns=['image', 'class'])\n",
    "    results_df.to_csv('results.csv',sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_loader, test_loader = create_datasets_dataloaders(train_images, train_labels)\n",
    "# cnn = ResNetMine(Bottleneck, [1, 1, 1, 1]).cuda()\n",
    "# trained_model = train(cnn, train_loader, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##predict on testset\n",
    "\n",
    "# test_transforms = transforms. Compose([\n",
    "#         transforms.CenterCrop(64),\n",
    "#         transforms.ToTensor()\n",
    "#     ])\n",
    "\n",
    "# test_dataset = ListsTestDataset(test_images, transform = test_transforms)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 100, shuffle = False)\n",
    "\n",
    "# predict_test_set(trained_model, test_loader, test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, test_loader, num_epochs):\n",
    "    learning_rate = 0.0001\n",
    "    weight_decay = 0.00005\n",
    "    batch_size = train_loader.batch_size\n",
    "    criterion = nn.CrossEntropyLoss();\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay);\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, 'max', factor=0.5, patience=5, verbose=True)\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate);\n",
    "    #Training\n",
    "    history = {'batch': [], 'loss': [], 'accuracy': []}\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train().cuda()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = Variable(images).cuda()\n",
    "            labels = Variable(labels).squeeze(1).long().cuda()#.cpu()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, argmax = torch.max(outputs, 1)\n",
    "            accuracy_train = (labels == argmax.squeeze()).float().mean()*100\n",
    "            # Show progress\n",
    "            if (i+1) % 32 == 0:\n",
    "                log = \" \".join([\n",
    "                  \"Epoch : %d/%d\" % (epoch+1, num_epochs),\n",
    "                  \"Iter : %d/%d\" % (i+1, len(train_loader.dataset)//batch_size),\n",
    "                  \"Loss: %.4f\" % loss.item(),\n",
    "                  \"Accuracy: %.4f\" % accuracy_train])\n",
    "                print('\\r{}'.format(log), end='')\n",
    "                history['batch'].append(i)\n",
    "                history['loss'].append(loss.item())\n",
    "                history['accuracy'].append(accuracy_train.item())\n",
    "        print()\n",
    "        ##VALIDATION SCORE AFTER EVERY EPOCH\n",
    "        model.eval().cuda()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = Variable(images).cuda()\n",
    "            labels= labels.squeeze(1)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.cpu().long() == labels).sum()\n",
    "        print('VALIDATION SET ACCURACY: %.4f %%' % (100*correct.item() / total))\n",
    "        scheduler.step(correct.item() / total)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]             576\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "              ReLU-3           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-4           [-1, 64, 16, 16]               0\n",
      "            Conv2d-5           [-1, 64, 16, 16]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 16, 16]             128\n",
      "              ReLU-7           [-1, 64, 16, 16]               0\n",
      "            Conv2d-8           [-1, 64, 16, 16]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
      "             ReLU-10           [-1, 64, 16, 16]               0\n",
      "           Conv2d-11          [-1, 256, 16, 16]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 16, 16]             512\n",
      "           Conv2d-13          [-1, 256, 16, 16]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 16, 16]             512\n",
      "             ReLU-15          [-1, 256, 16, 16]               0\n",
      "       Bottleneck-16          [-1, 256, 16, 16]               0\n",
      "           Conv2d-17           [-1, 64, 16, 16]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 16, 16]             128\n",
      "             ReLU-19           [-1, 64, 16, 16]               0\n",
      "           Conv2d-20           [-1, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 16, 16]             128\n",
      "             ReLU-22           [-1, 64, 16, 16]               0\n",
      "           Conv2d-23          [-1, 256, 16, 16]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 16, 16]             512\n",
      "             ReLU-25          [-1, 256, 16, 16]               0\n",
      "       Bottleneck-26          [-1, 256, 16, 16]               0\n",
      "        AvgPool2d-27            [-1, 256, 7, 7]               0\n",
      "           Linear-28                  [-1, 121]       1,517,945\n",
      "================================================================\n",
      "Total params: 1,664,057\n",
      "Trainable params: 1,664,057\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 8.22\n",
      "Params size (MB): 6.35\n",
      "Estimated Total Size (MB): 14.59\n",
      "----------------------------------------------------------------\n",
      "Epoch : 1/150 Iter : 672/680 Loss: 2.5766 Accuracy: 28.1250\n",
      "VALIDATION SET ACCURACY: 35.7703 %\n",
      "Epoch : 2/150 Iter : 672/680 Loss: 1.9741 Accuracy: 43.7500\n",
      "VALIDATION SET ACCURACY: 37.9182 %\n",
      "Epoch : 3/150 Iter : 672/680 Loss: 2.0924 Accuracy: 46.8750\n",
      "VALIDATION SET ACCURACY: 46.4271 %\n",
      "Epoch : 4/150 Iter : 672/680 Loss: 1.9762 Accuracy: 43.7500\n",
      "VALIDATION SET ACCURACY: 48.6576 %\n",
      "Epoch : 5/150 Iter : 672/680 Loss: 1.3469 Accuracy: 59.3750\n",
      "VALIDATION SET ACCURACY: 47.8315 %\n",
      "Epoch : 6/150 Iter : 672/680 Loss: 1.6896 Accuracy: 46.8750\n",
      "VALIDATION SET ACCURACY: 51.7555 %\n",
      "Epoch : 7/150 Iter : 672/680 Loss: 1.7543 Accuracy: 46.8750\n",
      "VALIDATION SET ACCURACY: 52.0446 %\n",
      "Epoch : 8/150 Iter : 672/680 Loss: 1.1281 Accuracy: 53.1250\n",
      "VALIDATION SET ACCURACY: 53.1185 %\n",
      "Epoch : 9/150 Iter : 672/680 Loss: 1.4190 Accuracy: 62.5000\n",
      "VALIDATION SET ACCURACY: 55.3903 %\n",
      "Epoch : 10/150 Iter : 672/680 Loss: 1.7665 Accuracy: 59.3750\n",
      "VALIDATION SET ACCURACY: 55.5143 %\n",
      "Epoch : 11/150 Iter : 672/680 Loss: 1.4624 Accuracy: 62.5000\n",
      "VALIDATION SET ACCURACY: 56.2577 %\n",
      "Epoch : 12/150 Iter : 672/680 Loss: 1.6529 Accuracy: 56.2500\n",
      "VALIDATION SET ACCURACY: 54.4403 %\n",
      "Epoch : 13/150 Iter : 672/680 Loss: 1.2203 Accuracy: 62.5000\n",
      "VALIDATION SET ACCURACY: 55.8034 %\n",
      "Epoch : 14/150 Iter : 672/680 Loss: 1.9193 Accuracy: 40.6250\n",
      "VALIDATION SET ACCURACY: 54.3164 %\n",
      "Epoch : 15/150 Iter : 672/680 Loss: 1.5461 Accuracy: 65.6250\n",
      "VALIDATION SET ACCURACY: 55.0599 %\n",
      "Epoch : 16/150 Iter : 672/680 Loss: 0.8618 Accuracy: 75.0000\n",
      "VALIDATION SET ACCURACY: 59.0252 %\n",
      "Epoch : 17/150 Iter : 672/680 Loss: 1.4091 Accuracy: 59.3750\n",
      "VALIDATION SET ACCURACY: 53.9447 %\n",
      "Epoch : 18/150 Iter : 672/680 Loss: 1.1752 Accuracy: 65.6250\n",
      "VALIDATION SET ACCURACY: 58.5295 %\n",
      "Epoch : 19/150 Iter : 672/680 Loss: 1.2286 Accuracy: 65.6250\n",
      "VALIDATION SET ACCURACY: 61.0078 %\n",
      "Epoch : 20/150 Iter : 672/680 Loss: 1.4030 Accuracy: 59.3750\n",
      "VALIDATION SET ACCURACY: 57.4969 %\n",
      "Epoch : 21/150 Iter : 672/680 Loss: 1.7342 Accuracy: 53.1250\n",
      "VALIDATION SET ACCURACY: 59.5209 %\n",
      "Epoch : 22/150 Iter : 672/680 Loss: 1.5098 Accuracy: 68.7500\n",
      "VALIDATION SET ACCURACY: 61.1731 %\n",
      "Epoch : 23/150 Iter : 672/680 Loss: 0.7753 Accuracy: 78.1250\n",
      "VALIDATION SET ACCURACY: 57.8686 %\n",
      "Epoch : 24/150 Iter : 672/680 Loss: 0.9942 Accuracy: 65.6250\n",
      "VALIDATION SET ACCURACY: 60.3470 %\n",
      "Epoch : 25/150 Iter : 672/680 Loss: 1.2273 Accuracy: 65.6250\n",
      "VALIDATION SET ACCURACY: 60.2644 %\n",
      "Epoch : 26/150 Iter : 672/680 Loss: 1.1308 Accuracy: 71.8750\n",
      "VALIDATION SET ACCURACY: 61.0905 %\n",
      "Epoch : 27/150 Iter : 672/680 Loss: 1.2532 Accuracy: 53.1250\n",
      "VALIDATION SET ACCURACY: 61.3796 %\n",
      "Epoch : 28/150 Iter : 672/680 Loss: 1.1682 Accuracy: 65.6250\n",
      "VALIDATION SET ACCURACY: 62.2883 %\n",
      "Epoch : 29/150 Iter : 672/680 Loss: 1.6722 Accuracy: 53.1250\n",
      "VALIDATION SET ACCURACY: 61.0492 %\n",
      "Epoch : 30/150 Iter : 672/680 Loss: 1.1020 Accuracy: 53.1250\n",
      "VALIDATION SET ACCURACY: 61.3383 %\n",
      "Epoch : 31/150 Iter : 672/680 Loss: 1.1594 Accuracy: 59.3750\n",
      "VALIDATION SET ACCURACY: 60.7187 %\n",
      "Epoch : 32/150 Iter : 672/680 Loss: 1.0549 Accuracy: 56.2500\n",
      "VALIDATION SET ACCURACY: 62.4535 %\n",
      "Epoch : 33/150 Iter : 672/680 Loss: 1.3576 Accuracy: 68.7500\n",
      "VALIDATION SET ACCURACY: 61.9992 %\n",
      "Epoch : 34/150 Iter : 672/680 Loss: 1.0398 Accuracy: 68.7500\n",
      "VALIDATION SET ACCURACY: 61.9579 %\n",
      "Epoch : 35/150 Iter : 672/680 Loss: 1.6121 Accuracy: 46.8750\n",
      "VALIDATION SET ACCURACY: 65.3449 %\n",
      "Epoch : 36/150 Iter : 672/680 Loss: 0.9198 Accuracy: 68.7500\n",
      "VALIDATION SET ACCURACY: 61.1318 %\n",
      "Epoch : 37/150 Iter : 672/680 Loss: 1.1794 Accuracy: 65.6250\n",
      "VALIDATION SET ACCURACY: 61.5035 %\n",
      "Epoch : 38/150 Iter : 672/680 Loss: 1.2203 Accuracy: 65.6250\n",
      "VALIDATION SET ACCURACY: 63.8579 %\n",
      "Epoch : 39/150 Iter : 672/680 Loss: 0.7940 Accuracy: 75.0000\n",
      "VALIDATION SET ACCURACY: 62.3709 %\n",
      "Epoch : 40/150 Iter : 672/680 Loss: 0.9942 Accuracy: 75.0000\n",
      "VALIDATION SET ACCURACY: 62.7840 %\n",
      "Epoch : 41/150 Iter : 672/680 Loss: 1.1963 Accuracy: 68.7500\n",
      "VALIDATION SET ACCURACY: 64.5601 %\n",
      "Epoch    40: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch : 42/150 Iter : 672/680 Loss: 0.7587 Accuracy: 78.1250\n",
      "VALIDATION SET ACCURACY: 65.1797 %\n",
      "Epoch : 43/150 Iter : 672/680 Loss: 0.8102 Accuracy: 71.8750\n",
      "VALIDATION SET ACCURACY: 64.9318 %\n",
      "Epoch : 44/150 Iter : 672/680 Loss: 1.0137 Accuracy: 62.5000\n",
      "VALIDATION SET ACCURACY: 65.5927 %\n",
      "Epoch : 45/150 Iter : 672/680 Loss: 0.8716 Accuracy: 71.8750\n",
      "VALIDATION SET ACCURACY: 65.9232 %\n",
      "Epoch : 46/150 Iter : 672/680 Loss: 1.1098 Accuracy: 65.6250\n",
      "VALIDATION SET ACCURACY: 65.5101 %\n",
      "Epoch : 47/150 Iter : 672/680 Loss: 0.7089 Accuracy: 71.8750\n",
      "VALIDATION SET ACCURACY: 65.2623 %\n",
      "Epoch : 48/150 Iter : 672/680 Loss: 0.7636 Accuracy: 81.2500\n",
      "VALIDATION SET ACCURACY: 65.2623 %\n",
      "Epoch : 49/150 Iter : 672/680 Loss: 0.7281 Accuracy: 75.0000\n",
      "VALIDATION SET ACCURACY: 66.0471 %\n",
      "Epoch : 50/150 Iter : 672/680 Loss: 0.9742 Accuracy: 62.5000\n",
      "VALIDATION SET ACCURACY: 64.3536 %\n",
      "Epoch : 51/150 Iter : 672/680 Loss: 0.6282 Accuracy: 87.5000\n",
      "VALIDATION SET ACCURACY: 64.7253 %\n",
      "Epoch : 52/150 Iter : 672/680 Loss: 0.6374 Accuracy: 84.3750\n",
      "VALIDATION SET ACCURACY: 65.3036 %\n",
      "Epoch : 53/150 Iter : 672/680 Loss: 1.2805 Accuracy: 56.2500\n",
      "VALIDATION SET ACCURACY: 64.8079 %\n",
      "Epoch : 54/150 Iter : 672/680 Loss: 1.1721 Accuracy: 56.2500\n",
      "VALIDATION SET ACCURACY: 65.9645 %\n",
      "Epoch : 55/150 Iter : 672/680 Loss: 1.1558 Accuracy: 65.6250\n",
      "VALIDATION SET ACCURACY: 65.5927 %\n",
      "Epoch    54: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch : 56/150 Iter : 672/680 Loss: 0.9264 Accuracy: 75.0000\n",
      "VALIDATION SET ACCURACY: 66.4601 %\n",
      "Epoch : 57/150 Iter : 672/680 Loss: 0.8493 Accuracy: 84.3750\n",
      "VALIDATION SET ACCURACY: 66.3362 %\n",
      "Epoch : 58/150 Iter : 672/680 Loss: 0.7136 Accuracy: 78.1250\n",
      "VALIDATION SET ACCURACY: 66.2536 %\n",
      "Epoch : 59/150 Iter : 672/680 Loss: 0.9750 Accuracy: 68.7500\n",
      "VALIDATION SET ACCURACY: 66.0058 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 60/150 Iter : 672/680 Loss: 0.9458 Accuracy: 68.7500\n",
      "VALIDATION SET ACCURACY: 66.8732 %\n",
      "Epoch : 61/150 Iter : 672/680 Loss: 0.7281 Accuracy: 75.0000\n",
      "VALIDATION SET ACCURACY: 67.4102 %\n",
      "Epoch : 62/150 Iter : 672/680 Loss: 0.7054 Accuracy: 75.0000\n",
      "VALIDATION SET ACCURACY: 66.9145 %\n",
      "Epoch : 63/150 Iter : 672/680 Loss: 0.7215 Accuracy: 78.1250\n",
      "VALIDATION SET ACCURACY: 66.7080 %\n",
      "Epoch : 64/150 Iter : 672/680 Loss: 0.7863 Accuracy: 75.0000\n",
      "VALIDATION SET ACCURACY: 66.9145 %\n",
      "Epoch : 65/150 Iter : 672/680 Loss: 0.7192 Accuracy: 71.8750\n",
      "VALIDATION SET ACCURACY: 65.7580 %\n",
      "Epoch : 66/150 Iter : 672/680 Loss: 0.7904 Accuracy: 65.6250\n",
      "VALIDATION SET ACCURACY: 66.8319 %\n",
      "Epoch : 67/150 Iter : 672/680 Loss: 0.3290 Accuracy: 87.5000\n",
      "VALIDATION SET ACCURACY: 66.9145 %\n",
      "Epoch    66: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch : 68/150 Iter : 672/680 Loss: 0.8269 Accuracy: 81.2500\n",
      "VALIDATION SET ACCURACY: 67.3276 %\n",
      "Epoch : 69/150 Iter : 672/680 Loss: 1.3456 Accuracy: 53.1250\n",
      "VALIDATION SET ACCURACY: 66.9558 %\n",
      "Epoch : 70/150 Iter : 672/680 Loss: 0.9073 Accuracy: 59.3750\n",
      "VALIDATION SET ACCURACY: 67.8645 %\n",
      "Epoch : 71/150 Iter : 672/680 Loss: 0.7069 Accuracy: 75.0000\n",
      "VALIDATION SET ACCURACY: 67.5341 %\n",
      "Epoch : 72/150 Iter : 672/680 Loss: 0.9193 Accuracy: 75.0000\n",
      "VALIDATION SET ACCURACY: 68.4015 %\n",
      "Epoch : 73/150 Iter : 672/680 Loss: 0.8431 Accuracy: 71.8750\n",
      "VALIDATION SET ACCURACY: 67.0797 %\n",
      "Epoch : 74/150 Iter : 672/680 Loss: 0.6882 Accuracy: 78.1250\n",
      "VALIDATION SET ACCURACY: 67.2036 %\n",
      "Epoch : 75/150 Iter : 192/680 Loss: 0.7003 Accuracy: 75.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4612789b879f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#     print(summary(cnn, (1,28,28)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mtrained_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-59ed224a4d7a>\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, test_loader, num_epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.cpu()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-41ab838e9ddf>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Transform image to tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mimg_as_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Return image and the label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_as_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_image_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mangle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mrotate\u001b[0;34m(img, angle, resample, expand, center)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img should be PIL Image. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mrotate\u001b[0;34m(self, angle, resample, expand, center, translate, fillcolor)\u001b[0m\n\u001b[1;32m   1888\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAFFINE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfillcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfillcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, size, method, data, resample, fill, fillcolor)\u001b[0m\n\u001b[1;32m   2171\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2173\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"getdata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2174\u001b[0m             \u001b[0;31m# compatibility w. old-style transform objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2175\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import NNs\n",
    "import math\n",
    "importlib.reload(NNs)\n",
    "from NNs import *\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True)\n",
    "trained_models = []\n",
    "for train_indexes, validation_indexes in kf.split(train_images):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    \n",
    "    for i in train_indexes:\n",
    "        X_train.append(train_images[i])\n",
    "        y_train.append(train_labels[i])\n",
    "    for j in validation_indexes:\n",
    "        X_val.append(train_images[j])\n",
    "        y_val.append(train_labels[j])\n",
    "    train_loader, test_loader = create_datasets_dataloaders(\n",
    "        X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    #Training\n",
    "    cnn = ResNetMine(Bottleneck, [2, 2, 2]).cuda()\n",
    "#     cnn = CNN().cuda()\n",
    "    summary(cnn, (1,64,64))\n",
    "\n",
    "#     print(summary(cnn, (1,28,28)))\n",
    "    trained_model = train_and_validate(cnn, train_loader, test_loader, num_epochs=150)\n",
    "    trained_models.append(trained_model)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = trained_models[0].eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on testset\n",
    "def predict_test_set(model, loader, filenames):\n",
    "    predictions = []\n",
    "    for images in loader:\n",
    "        images = Variable(images).cuda()\n",
    "        outputs = model(images)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        predictions.extend(prediction.cpu().numpy())\n",
    "    results_df = pd.DataFrame({'image': test_filenames, 'class': predictions}, columns=['image', 'class'])\n",
    "    results_df.to_csv('results.csv',sep = ',', index = False)\n",
    "\n",
    "\n",
    "test_transforms = transforms. Compose([\n",
    "        transforms.CenterCrop(64),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "test_dataset = ListsTestDataset(test_images, transform = test_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 100, shuffle = False)\n",
    "\n",
    "predict_test_set(final_model, test_loader, test_filenames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow - GPU",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
