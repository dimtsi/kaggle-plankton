{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchsummary import summary\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.12 s, sys: 481 ms, total: 4.6 s\n",
      "Wall time: 7.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#labels with the same order\n",
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "test_images = []\n",
    "test_dict = {}\n",
    "train_filenames = []\n",
    "\n",
    "labels_df = pd.read_csv('train_onelabel.csv')\n",
    "labels_dict = labels_df.set_index('image')['class'].to_dict()\n",
    "\n",
    "for filename in labels_df['image'].values: ##to keep mapping with classes\n",
    "    train_images.append(Image.open('train_images/'+filename).copy())\n",
    "    train_labels.append(labels_dict[filename])\n",
    "    train_filenames.append(filename)\n",
    "for filename in glob.iglob('test_images' +'/*'):\n",
    "    image = Image.open(filename).copy()\n",
    "    test_images.append(image)\n",
    "    test_dict[filename.replace('test_images/', '')] = image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average width 73.32110394976037 , Average height: 66.46897207073211\n",
      "CPU times: user 61.9 ms, sys: 0 ns, total: 61.9 ms\n",
      "Wall time: 70.2 ms\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "widths, heights = [], [] \n",
    "sumx, sumy = 0, 0\n",
    "for i in train_images:\n",
    "    sumx += i.size[0]\n",
    "    widths.append(i.size[0])\n",
    "    sumy += i.size[1]\n",
    "    heights.append(i.size[1])\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.hist(widths)\n",
    "ax2.hist(heights, color = 'orange')\n",
    "fig.set_size_inches(12, 5)\n",
    "\n",
    "avg_width = np.mean(widths)\n",
    "avg_height = np.mean(heights)\n",
    "print('Average width {} , Average height: {}'.format(avg_width, avg_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListsDataset(Dataset):\n",
    "    def __init__(self, list_of_images, list_of_labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            height (int): image height\n",
    "            width (int): image width\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "#         super().__init__()\n",
    "        self.data = list_of_images\n",
    "        self.labels = np.asarray(list_of_labels).reshape(-1,1)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        single_image = self.data[index]\n",
    "        single_image_label = self.labels[index]\n",
    "        # Transform image to tensor\n",
    "        if self.transform is not None:\n",
    "            img_as_tensor = self.transform(single_image)\n",
    "        # Return image and the label\n",
    "        return (img_as_tensor, single_image_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms and Dataset Creation\n",
    "test_transforms = transforms. Compose([transforms.ToTensor()])\n",
    "train_transforms = transforms. Compose([\n",
    "    transforms.CenterCrop(28),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = ListsDataset(train_images, train_labels, transform = train_transforms)\n",
    "test_dataset = ListsDataset(test_images, list_of_labels = None, transform = test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_images, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(50, 20, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(20*7*7, 250),\n",
    "            nn.ReLU())\n",
    "        self.fc = nn.Linear(250, 121)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs):\n",
    "    criterion = nn.CrossEntropyLoss();\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate);\n",
    "    #Training\n",
    "    history = {'batch': [], 'loss': [], 'accuracy': []}\n",
    "    for epoch in range(num_epochs):\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                images = Variable(images).cuda()\n",
    "                labels = Variable(labels).squeeze(1).long().cuda()#.cpu()\n",
    "                # Forward + Backward + Optimize\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _, argmax = torch.max(outputs, 1)\n",
    "                accuracy_train = (labels == argmax.squeeze()).float().mean()*100\n",
    "                # Show progress\n",
    "                if (i+1) % 32 == 0:\n",
    "                    log = \" \".join([\n",
    "                      \"Epoch : %d/%d\" % (epoch+1, num_epochs),\n",
    "                      \"Iter : %d/%d\" % (i+1, len(train_dataset)//batch_size),\n",
    "                      \"Loss: %.4f\" % loss.item(),\n",
    "                      \"Accuracy: %.4f\" % accuracy_train])\n",
    "                    print('\\r{}'.format(log), end='')\n",
    "                    history['batch'].append(i)\n",
    "                    history['loss'].append(loss.item())\n",
    "                    history['accuracy'].append(accuracy_train.item())\n",
    "            print()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 50, 28, 28]             500\n",
      "              ReLU-2           [-1, 50, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 50, 14, 14]               0\n",
      "            Conv2d-4           [-1, 20, 14, 14]           9,020\n",
      "              ReLU-5           [-1, 20, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 20, 7, 7]               0\n",
      "           Flatten-7                  [-1, 980]               0\n",
      "            Linear-8                  [-1, 250]         245,250\n",
      "              ReLU-9                  [-1, 250]               0\n",
      "           Linear-10                  [-1, 121]          30,371\n",
      "================================================================\n",
      "Total params: 285,141\n",
      "Trainable params: 285,141\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.75\n",
      "Params size (MB): 1.09\n",
      "Estimated Total Size (MB): 1.84\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN().cuda()\n",
    "summary(cnn, (1,28,28))\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/100 Iter : 736/756 Loss: 2.2086 Accuracy: 59.3750\n",
      "Epoch : 2/100 Iter : 736/756 Loss: 3.0338 Accuracy: 18.7500\n",
      "Epoch : 3/100 Iter : 736/756 Loss: 2.0770 Accuracy: 37.5000\n",
      "Epoch : 4/100 Iter : 736/756 Loss: 1.3751 Accuracy: 59.3750\n",
      "Epoch : 5/100 Iter : 736/756 Loss: 1.7285 Accuracy: 59.3750\n",
      "Epoch : 6/100 Iter : 736/756 Loss: 2.2140 Accuracy: 40.6250\n",
      "Epoch : 7/100 Iter : 736/756 Loss: 1.2820 Accuracy: 71.8750\n",
      "Epoch : 8/100 Iter : 736/756 Loss: 1.5642 Accuracy: 56.2500\n",
      "Epoch : 9/100 Iter : 736/756 Loss: 2.6918 Accuracy: 46.8750\n",
      "Epoch : 10/100 Iter : 736/756 Loss: 1.8689 Accuracy: 40.6250\n",
      "Epoch : 11/100 Iter : 736/756 Loss: 1.6346 Accuracy: 53.1250\n",
      "Epoch : 12/100 Iter : 736/756 Loss: 1.0903 Accuracy: 59.3750\n",
      "Epoch : 13/100 Iter : 736/756 Loss: 1.4116 Accuracy: 50.0000\n",
      "Epoch : 14/100 Iter : 736/756 Loss: 1.9391 Accuracy: 50.0000\n",
      "Epoch : 15/100 Iter : 736/756 Loss: 1.3172 Accuracy: 68.7500\n",
      "Epoch : 16/100 Iter : 736/756 Loss: 1.5430 Accuracy: 56.2500\n",
      "Epoch : 17/100 Iter : 736/756 Loss: 1.0453 Accuracy: 62.5000\n",
      "Epoch : 18/100 Iter : 736/756 Loss: 1.3333 Accuracy: 59.3750\n",
      "Epoch : 19/100 Iter : 736/756 Loss: 0.8815 Accuracy: 78.1250\n",
      "Epoch : 20/100 Iter : 736/756 Loss: 1.2124 Accuracy: 68.7500\n",
      "Epoch : 21/100 Iter : 736/756 Loss: 0.8961 Accuracy: 71.8750\n",
      "Epoch : 22/100 Iter : 736/756 Loss: 2.0064 Accuracy: 50.0000\n",
      "Epoch : 23/100 Iter : 736/756 Loss: 1.0467 Accuracy: 65.6250\n",
      "Epoch : 24/100 Iter : 736/756 Loss: 1.0274 Accuracy: 65.6250\n",
      "Epoch : 25/100 Iter : 736/756 Loss: 0.7915 Accuracy: 71.8750\n",
      "Epoch : 26/100 Iter : 736/756 Loss: 0.6618 Accuracy: 84.3750\n",
      "Epoch : 27/100 Iter : 736/756 Loss: 0.7601 Accuracy: 78.1250\n",
      "Epoch : 28/100 Iter : 736/756 Loss: 1.2570 Accuracy: 59.3750\n",
      "Epoch : 29/100 Iter : 736/756 Loss: 1.4933 Accuracy: 71.8750\n",
      "Epoch : 30/100 Iter : 736/756 Loss: 0.5527 Accuracy: 78.1250\n",
      "Epoch : 31/100 Iter : 736/756 Loss: 0.8628 Accuracy: 68.7500\n",
      "Epoch : 32/100 Iter : 736/756 Loss: 0.8427 Accuracy: 71.8750\n",
      "Epoch : 33/100 Iter : 736/756 Loss: 1.0592 Accuracy: 68.7500\n",
      "Epoch : 34/100 Iter : 736/756 Loss: 0.5298 Accuracy: 78.1250\n",
      "Epoch : 35/100 Iter : 736/756 Loss: 0.3354 Accuracy: 90.6250\n",
      "Epoch : 36/100 Iter : 736/756 Loss: 0.9354 Accuracy: 68.7500\n",
      "Epoch : 37/100 Iter : 736/756 Loss: 0.5051 Accuracy: 87.5000\n",
      "Epoch : 38/100 Iter : 736/756 Loss: 0.5424 Accuracy: 75.0000\n",
      "Epoch : 39/100 Iter : 736/756 Loss: 0.7119 Accuracy: 81.2500\n",
      "Epoch : 40/100 Iter : 736/756 Loss: 0.8547 Accuracy: 75.0000\n",
      "Epoch : 41/100 Iter : 736/756 Loss: 0.8783 Accuracy: 75.0000\n",
      "Epoch : 42/100 Iter : 736/756 Loss: 0.5849 Accuracy: 71.8750\n",
      "Epoch : 43/100 Iter : 736/756 Loss: 0.6449 Accuracy: 84.3750\n",
      "Epoch : 44/100 Iter : 736/756 Loss: 0.6666 Accuracy: 84.3750\n",
      "Epoch : 45/100 Iter : 736/756 Loss: 0.3630 Accuracy: 87.5000\n",
      "Epoch : 46/100 Iter : 736/756 Loss: 0.5050 Accuracy: 90.6250\n",
      "Epoch : 47/100 Iter : 736/756 Loss: 0.3795 Accuracy: 90.6250\n",
      "Epoch : 48/100 Iter : 736/756 Loss: 0.9269 Accuracy: 62.5000\n",
      "Epoch : 49/100 Iter : 736/756 Loss: 0.5004 Accuracy: 81.2500\n",
      "Epoch : 50/100 Iter : 736/756 Loss: 0.4194 Accuracy: 87.5000\n",
      "Epoch : 51/100 Iter : 736/756 Loss: 0.3670 Accuracy: 84.3750\n",
      "Epoch : 52/100 Iter : 736/756 Loss: 0.3948 Accuracy: 87.5000\n",
      "Epoch : 53/100 Iter : 736/756 Loss: 0.3216 Accuracy: 84.3750\n",
      "Epoch : 54/100 Iter : 736/756 Loss: 0.6975 Accuracy: 75.0000\n",
      "Epoch : 55/100 Iter : 736/756 Loss: 0.3019 Accuracy: 90.6250\n",
      "Epoch : 56/100 Iter : 736/756 Loss: 0.5767 Accuracy: 87.50000\n",
      "Epoch : 57/100 Iter : 736/756 Loss: 0.2865 Accuracy: 93.7500\n",
      "Epoch : 58/100 Iter : 736/756 Loss: 0.3516 Accuracy: 90.6250\n",
      "Epoch : 59/100 Iter : 736/756 Loss: 0.4602 Accuracy: 78.12500\n",
      "Epoch : 60/100 Iter : 736/756 Loss: 0.5125 Accuracy: 81.25000\n",
      "Epoch : 61/100 Iter : 736/756 Loss: 0.4919 Accuracy: 90.6250\n",
      "Epoch : 62/100 Iter : 736/756 Loss: 0.2992 Accuracy: 90.6250\n",
      "Epoch : 63/100 Iter : 736/756 Loss: 0.1487 Accuracy: 93.7500\n",
      "Epoch : 64/100 Iter : 736/756 Loss: 0.2859 Accuracy: 93.75000\n",
      "Epoch : 65/100 Iter : 736/756 Loss: 0.5508 Accuracy: 78.12500\n",
      "Epoch : 66/100 Iter : 736/756 Loss: 0.6214 Accuracy: 84.37500\n",
      "Epoch : 67/100 Iter : 736/756 Loss: 0.3673 Accuracy: 93.7500\n",
      "Epoch : 68/100 Iter : 736/756 Loss: 0.1893 Accuracy: 93.7500\n",
      "Epoch : 69/100 Iter : 736/756 Loss: 0.3157 Accuracy: 90.62500\n",
      "Epoch : 70/100 Iter : 736/756 Loss: 0.1959 Accuracy: 93.75000\n",
      "Epoch : 71/100 Iter : 736/756 Loss: 0.2623 Accuracy: 90.62500\n",
      "Epoch : 72/100 Iter : 736/756 Loss: 0.2470 Accuracy: 93.75000\n",
      "Epoch : 73/100 Iter : 736/756 Loss: 0.2191 Accuracy: 90.6250\n",
      "Epoch : 74/100 Iter : 736/756 Loss: 0.0389 Accuracy: 100.0000\n",
      "Epoch : 75/100 Iter : 736/756 Loss: 0.2846 Accuracy: 90.62500\n",
      "Epoch : 76/100 Iter : 736/756 Loss: 0.4073 Accuracy: 84.3750\n",
      "Epoch : 77/100 Iter : 736/756 Loss: 0.2239 Accuracy: 90.62500\n",
      "Epoch : 78/100 Iter : 736/756 Loss: 0.1995 Accuracy: 96.8750\n",
      "Epoch : 79/100 Iter : 736/756 Loss: 0.5712 Accuracy: 84.37500\n",
      "Epoch : 80/100 Iter : 736/756 Loss: 0.1599 Accuracy: 93.75000\n",
      "Epoch : 81/100 Iter : 736/756 Loss: 0.1947 Accuracy: 93.75000\n",
      "Epoch : 82/100 Iter : 736/756 Loss: 0.2393 Accuracy: 90.62500\n",
      "Epoch : 83/100 Iter : 736/756 Loss: 0.3276 Accuracy: 90.62500\n",
      "Epoch : 84/100 Iter : 736/756 Loss: 0.5084 Accuracy: 84.37500\n",
      "Epoch : 85/100 Iter : 736/756 Loss: 0.3960 Accuracy: 84.37500\n",
      "Epoch : 86/100 Iter : 736/756 Loss: 0.1532 Accuracy: 96.87500\n",
      "Epoch : 87/100 Iter : 736/756 Loss: 0.3472 Accuracy: 84.37500\n",
      "Epoch : 88/100 Iter : 736/756 Loss: 0.1626 Accuracy: 93.75000\n",
      "Epoch : 89/100 Iter : 736/756 Loss: 0.3262 Accuracy: 90.62500\n",
      "Epoch : 90/100 Iter : 736/756 Loss: 0.0521 Accuracy: 100.0000\n",
      "Epoch : 91/100 Iter : 736/756 Loss: 0.1425 Accuracy: 90.62500\n",
      "Epoch : 92/100 Iter : 736/756 Loss: 0.1346 Accuracy: 96.87500\n",
      "Epoch : 93/100 Iter : 736/756 Loss: 0.2111 Accuracy: 90.62500\n",
      "Epoch : 94/100 Iter : 736/756 Loss: 0.1282 Accuracy: 96.87500\n",
      "Epoch : 95/100 Iter : 736/756 Loss: 0.1918 Accuracy: 93.75000\n",
      "Epoch : 96/100 Iter : 736/756 Loss: 0.0450 Accuracy: 100.0000\n",
      "Epoch : 97/100 Iter : 736/756 Loss: 0.0735 Accuracy: 96.87500\n",
      "Epoch : 98/100 Iter : 736/756 Loss: 0.2417 Accuracy: 90.62500\n",
      "Epoch : 99/100 Iter : 736/756 Loss: 0.0986 Accuracy: 93.75000\n",
      "Epoch : 100/100 Iter : 736/756 Loss: 0.1304 Accuracy: 96.87500\n",
      "CPU times: user 6min 5s, sys: 41.2 s, total: 6min 46s\n",
      "Wall time: 6min 48s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(50, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Flatten()\n",
       "    (1): Linear(in_features=980, out_features=250, bias=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (fc): Linear(in_features=250, out_features=121, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cnn = train(cnn, num_epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "batch must contain tensors, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-017d059f3f76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: batch must contain tensors, numbers, dicts or lists; found <class 'PIL.Image.Image'>"
     ]
    }
   ],
   "source": [
    "cnn.eval().cuda()\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images)\n",
    "    labels= labels.squeeze(1)\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.float() == labels).sum()\n",
    "print('Test Accuracy of the model on the 60000 test images: %.4f %%' % (100*correct.item() / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow - GPU",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
