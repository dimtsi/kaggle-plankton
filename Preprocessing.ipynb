{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import math;\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchsummary import summary\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = pickle.load(open(\"pkl/preprocessed_train_images.pkl\", \"rb\"))\n",
    "train_labels = pickle.load(open(\"pkl/train_labels.pkl\", \"rb\"))\n",
    "train_filenames = pickle.load(open(\"pkl/train_filenames.pkl\", \"rb\"))\n",
    "test_images = pickle.load(open(\"pkl/preprocessed_test_images.pkl\", \"rb\"))\n",
    "test_filenames = pickle.load(open(\"pkl/test_filenames.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average width 64.0 , Average height: 64.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PIL\n",
    "\n",
    "widths, heights = [], [] \n",
    "sumx, sumy = 0, 0\n",
    "for i in train_images:\n",
    "    sumx += i.size[0]\n",
    "    widths.append(i.size[0])\n",
    "    sumy += i.size[1]\n",
    "    heights.append(i.size[1])\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.hist(widths)\n",
    "ax2.hist(heights, color = 'orange')\n",
    "fig.set_size_inches(12, 5)\n",
    "\n",
    "avg_width = np.mean(widths)\n",
    "avg_height = np.mean(heights)\n",
    "print('Average width {} , Average height: {}'.format(avg_width, avg_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListsTrainDataset(Dataset):\n",
    "    def __init__(self, list_of_images, list_of_labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            height (int): image height\n",
    "            width (int): image width\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "#         super().__init__()\n",
    "        self.data = list_of_images\n",
    "        self.labels = np.asarray(list_of_labels).reshape(-1,1)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        single_image = self.data[index]\n",
    "        single_image_label = self.labels[index]\n",
    "        # Transform image to tensor\n",
    "        if self.transform is not None:\n",
    "            img_as_tensor = self.transform(single_image)\n",
    "        # Return image and the label\n",
    "        return (img_as_tensor, single_image_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListsTestDataset(Dataset):\n",
    "    def __init__(self, list_of_images, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            height (int): image height\n",
    "            width (int): image width\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        self.data = list_of_images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        single_image = self.data[index]\n",
    "        if self.transform is not None:\n",
    "            img_as_tensor = self.transform(single_image)\n",
    "        # Return image ONLY\n",
    "        return img_as_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms and Dataset Creation\n",
    "def create_datasets_dataloaders(X_train, y_train, X_test= None, y_test = None):\n",
    "    test_transforms = transforms. Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    train_transforms = transforms. Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=360),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    train_dataset = ListsTrainDataset(X_train, y_train, transform = train_transforms)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 32, shuffle = True, num_workers=2)\n",
    "\n",
    "    if y_test is not None:\n",
    "        test_dataset = ListsTrainDataset(X_test, y_test, transform = test_transforms)\n",
    "    else:\n",
    "        test_dataset = ListsTestDataset(X_test, transform = test_transforms)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 32, shuffle = False)\n",
    "    return (train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import BasicBlock, Bottleneck, ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, num_epochs):\n",
    "    learning_rate = 0.0005\n",
    "    batch_size = data_loader.batch_size\n",
    "    criterion = nn.CrossEntropyLoss();\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate);\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate);\n",
    "    #Training\n",
    "    history = {'batch': [], 'loss': [], 'accuracy': []}\n",
    "    for epoch in range(num_epochs):\n",
    "            for i, (images, labels) in enumerate(data_loader):\n",
    "                images = Variable(images).cuda()\n",
    "                labels = Variable(labels).squeeze(1).long().cuda()#.cpu()\n",
    "                # Forward + Backward + Optimize\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _, argmax = torch.max(outputs, 1)\n",
    "                accuracy_train = (labels == argmax.squeeze()).float().mean()*100\n",
    "                # Show progress\n",
    "                if (i+1) % 32 == 0:\n",
    "                    log = \" \".join([\n",
    "                      \"Epoch : %d/%d\" % (epoch+1, num_epochs),\n",
    "                      \"Iter : %d/%d\" % (i+1, len(data_loader.dataset)//batch_size),\n",
    "                      \"Loss: %.4f\" % loss.item(),\n",
    "                      \"Accuracy: %.4f\" % accuracy_train])\n",
    "                    print('\\r{}'.format(log), end='')\n",
    "                    history['batch'].append(i)\n",
    "                    history['loss'].append(loss.item())\n",
    "                    history['accuracy'].append(accuracy_train.item())\n",
    "            print()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cnn.eval().cuda()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# for images, labels in test_loader:\n",
    "#     images = Variable(images)\n",
    "#     labels= labels.squeeze(1)\n",
    "#     outputs = cnn(images)\n",
    "#     _, predicted = torch.max(outputs.data, 1)\n",
    "#     total += labels.size(0)\n",
    "#     correct += (predicted.float() == labels).sum()\n",
    "# print('Test Accuracy of the model on the 60000 test images: %.4f %%' % (100*correct.item() / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def predict_test_set(model, loader, filenames):\n",
    "    predictions = []\n",
    "    for images in loader:\n",
    "        images = Variable(images).cuda()\n",
    "        outputs = model(images)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        predictions.extend(prediction.cpu().numpy())\n",
    "    results_df = pd.DataFrame({'image': test_filenames, 'class': predictions}, columns=['image', 'class'])\n",
    "    results_df.to_csv('results.csv',sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_loader, test_loader = create_datasets_dataloaders(train_images, train_labels)\n",
    "# cnn = ResNetMine(Bottleneck, [1, 1, 1, 1]).cuda()\n",
    "# trained_model = train(cnn, train_loader, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##predict on testset\n",
    "\n",
    "# test_transforms = transforms. Compose([\n",
    "#         transforms.CenterCrop(64),\n",
    "#         transforms.ToTensor()\n",
    "#     ])\n",
    "\n",
    "# test_dataset = ListsTestDataset(test_images, transform = test_transforms)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 100, shuffle = False)\n",
    "\n",
    "# predict_test_set(trained_model, test_loader, test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, test_loader, num_epochs):\n",
    "    learning_rate = 0.0001\n",
    "    weight_decay = 0.00005\n",
    "    batch_size = train_loader.batch_size\n",
    "    criterion = nn.CrossEntropyLoss();\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay);\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate);\n",
    "    #Training\n",
    "    history = {'batch': [], 'loss': [], 'accuracy': []}\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train().cuda()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = Variable(images).cuda()\n",
    "            labels = Variable(labels).squeeze(1).long().cuda()#.cpu()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, argmax = torch.max(outputs, 1)\n",
    "            accuracy_train = (labels == argmax.squeeze()).float().mean()*100\n",
    "            # Show progress\n",
    "            if (i+1) % 32 == 0:\n",
    "                log = \" \".join([\n",
    "                  \"Epoch : %d/%d\" % (epoch+1, num_epochs),\n",
    "                  \"Iter : %d/%d\" % (i+1, len(train_loader.dataset)//batch_size),\n",
    "                  \"Loss: %.4f\" % loss.item(),\n",
    "                  \"Accuracy: %.4f\" % accuracy_train])\n",
    "                print('\\r{}'.format(log), end='')\n",
    "                history['batch'].append(i)\n",
    "                history['loss'].append(loss.item())\n",
    "                history['accuracy'].append(accuracy_train.item())\n",
    "        print()\n",
    "        ##VALIDATION SCORE AFTER EVERY EPOCH\n",
    "        model.eval().cuda()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = Variable(images).cuda()\n",
    "            labels= labels.squeeze(1)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.cpu().long() == labels).sum()\n",
    "        print('VALIDATION SET ACCURACY: %.4f %%' % (100*correct.item() / total))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import NNs\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [32 x 5120], m2: [980 x 250] at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/THC/generic/THCTensorMathBlas.cu:249",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7dabc7415d32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#     print(summary(cnn, (1,28,28)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mtrained_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-06d5b37b9072>\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, test_loader, num_epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# Forward + Backward + Optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/NNs.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [32 x 5120], m2: [980 x 250] at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/THC/generic/THCTensorMathBlas.cu:249"
     ]
    }
   ],
   "source": [
    "importlib.reload(NNs)\n",
    "from NNs import *\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True)\n",
    "trained_models = []\n",
    "for train_indexes, validation_indexes in kf.split(train_images):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    \n",
    "    for i in train_indexes:\n",
    "        X_train.append(train_images[i])\n",
    "        y_train.append(train_labels[i])\n",
    "    for j in validation_indexes:\n",
    "        X_val.append(train_images[j])\n",
    "        y_val.append(train_labels[j])\n",
    "    train_loader, test_loader = create_datasets_dataloaders(\n",
    "        X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    #Training\n",
    "#     cnn = ResNetMine(BasicBlock, [1, 1, 1, 1]).cuda()\n",
    "    cnn = CNN().cuda()\n",
    "\n",
    "#     print(summary(cnn, (1,28,28)))\n",
    "    trained_model = train_and_validate(cnn, train_loader, test_loader, num_epochs=150)\n",
    "    trained_models.append(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetMine(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=121, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (1024x2x2). Calculated output size: (1024x0x0). Output size is too small at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/THCUNN/generic/SpatialAveragePooling.cu:63",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b737d9cbfa3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNetMine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBottleneck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/NNs.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m#         x = self.layer4(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         return F.avg_pool2d(input, self.kernel_size, self.stride,\n\u001b[0;32m--> 547\u001b[0;31m                             self.padding, self.ceil_mode, self.count_include_pad)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (1024x2x2). Calculated output size: (1024x0x0). Output size is too small at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/THCUNN/generic/SpatialAveragePooling.cu:63"
     ]
    }
   ],
   "source": [
    "# cnn2 = trained_models[1]\n",
    "# cnn2.fc.weight\n",
    "\n",
    "cnn = ResNetMine(Bottleneck, [1, 1, 1]).cuda()\n",
    "summary(cnn, (1,28,28))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-GPU",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
