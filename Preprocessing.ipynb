{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchsummary import summary\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 s, sys: 3.25 s, total: 15.3 s\n",
      "Wall time: 2min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#labels with the same order\n",
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "test_images = []\n",
    "test_dict = {}\n",
    "train_filenames = []\n",
    "test_filenames = []\n",
    "\n",
    "\n",
    "labels_df = pd.read_csv('train_onelabel.csv')\n",
    "labels_dict = labels_df.set_index('image')['class'].to_dict()\n",
    "\n",
    "for filename in labels_df['image'].values: ##to keep mapping with classes\n",
    "    train_images.append(Image.open('train_images/'+filename).copy())\n",
    "    train_labels.append(labels_dict[filename])\n",
    "    train_filenames.append(filename)\n",
    "for filename in glob.iglob('test_images' +'/*'):\n",
    "    image = Image.open(filename).copy()\n",
    "    test_images.append(image)\n",
    "    test_filenames.append(filename.replace('test_images/', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average width 73.32110394976037 , Average height: 66.46897207073211\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PIL\n",
    "\n",
    "widths, heights = [], [] \n",
    "sumx, sumy = 0, 0\n",
    "for i in train_images:\n",
    "    sumx += i.size[0]\n",
    "    widths.append(i.size[0])\n",
    "    sumy += i.size[1]\n",
    "    heights.append(i.size[1])\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.hist(widths)\n",
    "ax2.hist(heights, color = 'orange')\n",
    "fig.set_size_inches(12, 5)\n",
    "\n",
    "avg_width = np.mean(widths)\n",
    "avg_height = np.mean(heights)\n",
    "print('Average width {} , Average height: {}'.format(avg_width, avg_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListsTrainDataset(Dataset):\n",
    "    def __init__(self, list_of_images, list_of_labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            height (int): image height\n",
    "            width (int): image width\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "#         super().__init__()\n",
    "        self.data = list_of_images\n",
    "        self.labels = np.asarray(list_of_labels).reshape(-1,1)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        single_image = self.data[index]\n",
    "        single_image_label = self.labels[index]\n",
    "        # Transform image to tensor\n",
    "        if self.transform is not None:\n",
    "            img_as_tensor = self.transform(single_image)\n",
    "        # Return image and the label\n",
    "        return (img_as_tensor, single_image_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListsTestDataset(Dataset):\n",
    "    def __init__(self, list_of_images, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            height (int): image height\n",
    "            width (int): image width\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        self.data = list_of_images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        single_image = self.data[index]\n",
    "        if self.transform is not None:\n",
    "            img_as_tensor = self.transform(single_image)\n",
    "        # Return image ONLY\n",
    "        return img_as_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms and Dataset Creation\n",
    "def create_datasets_dataloaders(X_train, y_train, X_test, y_test = None):\n",
    "    test_transforms = transforms. Compose([\n",
    "        transforms.CenterCrop(28),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    train_transforms = transforms. Compose([\n",
    "        transforms.CenterCrop(28),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    train_dataset = ListsTrainDataset(X_train, y_train, transform = train_transforms)\n",
    "    if y_test is not None:\n",
    "        test_dataset = ListsTrainDataset(X_test, y_test, transform = test_transforms)\n",
    "    else:\n",
    "        test_dataset = ListsTestDataset(X_test, transform = test_transforms)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 100, shuffle = False)\n",
    "    return (train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(50, 20, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(20*7*7, 250),\n",
    "            nn.ReLU())\n",
    "        self.fc = nn.Linear(250, 121)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, num_epochs):\n",
    "    learning_rate = 0.0005\n",
    "    batch_size = data_loader.batch_size\n",
    "    criterion = nn.CrossEntropyLoss();\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate);\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate);\n",
    "    #Training\n",
    "    history = {'batch': [], 'loss': [], 'accuracy': []}\n",
    "    for epoch in range(num_epochs):\n",
    "            for i, (images, labels) in enumerate(data_loader):\n",
    "                images = Variable(images).cuda()\n",
    "                labels = Variable(labels).squeeze(1).long().cuda()#.cpu()\n",
    "                # Forward + Backward + Optimize\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _, argmax = torch.max(outputs, 1)\n",
    "                accuracy_train = (labels == argmax.squeeze()).float().mean()*100\n",
    "                # Show progress\n",
    "                if (i+1) % 32 == 0:\n",
    "                    log = \" \".join([\n",
    "                      \"Epoch : %d/%d\" % (epoch+1, num_epochs),\n",
    "                      \"Iter : %d/%d\" % (i+1, len(data_loader.dataset)//batch_size),\n",
    "                      \"Loss: %.4f\" % loss.item(),\n",
    "                      \"Accuracy: %.4f\" % accuracy_train])\n",
    "                    print('\\r{}'.format(log), end='')\n",
    "                    history['batch'].append(i)\n",
    "                    history['loss'].append(loss.item())\n",
    "                    history['accuracy'].append(accuracy_train.item())\n",
    "            print()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn.eval().cuda()\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images)\n",
    "    labels= labels.squeeze(1)\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.float() == labels).sum()\n",
    "print('Test Accuracy of the model on the 60000 test images: %.4f %%' % (100*correct.item() / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def predict_test_set(model, test_filenames, ):\n",
    "    predictions = []\n",
    "    for images in test_loader:\n",
    "        images = Variable(images).cuda()\n",
    "        outputs = cnn(images)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        predictions.extend(prediction.cpu().numpy())\n",
    "    results_df = pd.DataFrame({'image': test_filenames, 'class': predictions}, columns=['image', 'class'])\n",
    "    results_df.to_csv('results.csv',sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_sample = train_images[:1000]\n",
    "y_sample = train_labels[:1000]\n",
    "\n",
    "x_sample[675]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/5 Iter : 576/605 Loss: 2.8555 Accuracy: 31.2500\n",
      "Epoch : 2/5 Iter : 576/605 Loss: 2.0015 Accuracy: 46.8750\n",
      "Epoch : 3/5 Iter : 576/605 Loss: 2.2042 Accuracy: 46.8750\n",
      "Epoch : 4/5 Iter : 576/605 Loss: 2.2267 Accuracy: 46.8750\n",
      "Epoch : 5/5 Iter : 576/605 Loss: 1.7001 Accuracy: 59.3750\n",
      "Epoch : 1/5 Iter : 576/605 Loss: 3.1284 Accuracy: 31.2500\n",
      "Epoch : 2/5 Iter : 576/605 Loss: 2.2007 Accuracy: 59.3750\n",
      "Epoch : 3/5 Iter : 576/605 Loss: 2.4296 Accuracy: 40.6250\n",
      "Epoch : 4/5 Iter : 576/605 Loss: 2.3031 Accuracy: 43.7500\n",
      "Epoch : 5/5 Iter : 576/605 Loss: 1.6738 Accuracy: 43.7500\n",
      "Epoch : 1/5 Iter : 576/605 Loss: 2.9217 Accuracy: 28.1250\n",
      "Epoch : 2/5 Iter : 576/605 Loss: 2.4680 Accuracy: 37.5000\n",
      "Epoch : 3/5 Iter : 576/605 Loss: 2.7614 Accuracy: 25.0000\n",
      "Epoch : 4/5 Iter : 576/605 Loss: 2.3855 Accuracy: 31.2500\n",
      "Epoch : 5/5 Iter : 576/605 Loss: 2.1188 Accuracy: 37.5000\n",
      "Epoch : 1/5 Iter : 576/605 Loss: 3.4049 Accuracy: 18.7500\n",
      "Epoch : 2/5 Iter : 576/605 Loss: 2.3893 Accuracy: 37.5000\n",
      "Epoch : 3/5 Iter : 576/605 Loss: 1.9707 Accuracy: 53.1250\n",
      "Epoch : 4/5 Iter : 576/605 Loss: 2.4989 Accuracy: 31.2500\n",
      "Epoch : 5/5 Iter : 576/605 Loss: 1.8802 Accuracy: 50.0000\n",
      "Epoch : 1/5 Iter : 576/605 Loss: 3.2816 Accuracy: 21.8750\n",
      "Epoch : 2/5 Iter : 576/605 Loss: 2.6475 Accuracy: 34.3750\n",
      "Epoch : 3/5 Iter : 576/605 Loss: 2.6107 Accuracy: 34.3750\n",
      "Epoch : 4/5 Iter : 576/605 Loss: 2.2632 Accuracy: 43.7500\n",
      "Epoch : 5/5 Iter : 576/605 Loss: 2.4838 Accuracy: 34.3750\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "trained_models = []\n",
    "for train_indexes, validation_indexes in kf.split(train_images):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    \n",
    "    for i in train_indexes:\n",
    "        X_train.append(train_images[i])\n",
    "        y_train.append(train_labels[i])\n",
    "    for j in validation_indexes:\n",
    "        X_val.append(train_images[j])\n",
    "        y_val.append(train_labels[j])\n",
    "    train_loader, test_loader = create_datasets_dataloaders(\n",
    "        X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    #Training\n",
    "    cnn = CNN().cuda()\n",
    "#     print(summary(cnn, (1,28,28)))\n",
    "    trained_model = train(cnn, train_loader, num_epochs=100)\n",
    "    trained_models.append(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0168, -0.0130,  0.0252,  ...,  0.0460, -0.0444,  0.0676],\n",
       "        [-0.0317, -0.0941,  0.0557,  ...,  0.0234,  0.0467,  0.0618],\n",
       "        [-0.3464, -0.0394,  0.0285,  ...,  0.0470, -0.0223, -0.1115],\n",
       "        ...,\n",
       "        [-0.1316,  0.0347, -0.0653,  ...,  0.0428,  0.0268, -0.0213],\n",
       "        [ 0.0074,  0.0450, -0.0466,  ...,  0.0525,  0.0084,  0.0840],\n",
       "        [-0.0987, -0.0957, -0.0103,  ..., -0.0731, -0.0283, -0.0347]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn1 = trained_models[0]\n",
    "cnn1.fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0070,  0.0135,  0.0011,  ..., -0.0310, -0.0125, -0.0374],\n",
       "        [ 0.0209, -0.0459,  0.0046,  ...,  0.0476,  0.0577,  0.0305],\n",
       "        [-0.0291,  0.0265,  0.0575,  ...,  0.0408,  0.0601,  0.0046],\n",
       "        ...,\n",
       "        [-0.0418, -0.0169,  0.0464,  ..., -0.0072,  0.0393, -0.0152],\n",
       "        [ 0.0230,  0.0071,  0.0051,  ..., -0.0624,  0.0516, -0.0562],\n",
       "        [-0.0260, -0.0319,  0.0064,  ...,  0.0367,  0.0147,  0.0346]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn2 = trained_models[1]\n",
    "cnn2.fc.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-GPU",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
